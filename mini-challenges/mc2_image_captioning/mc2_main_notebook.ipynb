{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141c0c5d",
   "metadata": {},
   "source": [
    "# Deep Learning Mini-Challenge 2: Image Captioning\n",
    "\n",
    "**Task description:** \n",
    "\n",
    "**Description of the dataset:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "\n",
    "import wandb\n",
    "\n",
    "# print(torch.__version__)\n",
    "# print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../../data/Flickr8k/Images/\"\n",
    "label_path = \"../../data/Flickr8k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f322ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(label_path, skip_header=True):\n",
    "    '''\n",
    "    Reads the labels and caption text from the captions.txt file in the specified path\n",
    "    '''\n",
    "    with open(label_path + \"captions.txt\") as f:\n",
    "        if skip_header:\n",
    "            next(f)\n",
    "        lines = f.readlines()\n",
    "        lines = [line.replace(\"\\n\", \"\") for line in lines]\n",
    "        lines = [line.split(\".jpg,\") for line in lines]\n",
    "        filenames = [line[0] + \".jpg\" for line in lines]\n",
    "        text = [line[1] for line in lines]\n",
    "        return(pd.DataFrame([filenames, text], index=([\"filename\", \"text\"])).T)\n",
    "\n",
    "df_caption = read_labels(label_path)\n",
    "df_caption.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db09e0b",
   "metadata": {},
   "source": [
    "## Explorative data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_imeages(df, n=3, m=2):\n",
    "    '''\n",
    "    Visualises a number of images with the corresponding captions\n",
    "    '''\n",
    "    fig, axes = plt.subplots(n, m, figsize=(22,14))\n",
    "    unique_files = df_caption.filename.unique()\n",
    "\n",
    "    for i in range(n*m):\n",
    "        filename = unique_files[i]\n",
    "        caption = \"\\n\".join(list(df.loc[df[\"filename\"]==unique_files[i]][\"text\"]))\n",
    "        img = mpimg.imread(image_path + filename)\n",
    "        axes[i//m, i%m].imshow(img)\n",
    "        axes[i//m, i%m].set_title(caption)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_sample_imeages(df_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9327258",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.ecdfplot(df_caption.text.apply(str.split).apply(len))\n",
    "plt.title(\"ecdf of nr of words per caption\")\n",
    "plt.xlabel(\"nr of words\")\n",
    "plt.ylabel(\"proportion\")\n",
    "plt.show()\n",
    "\n",
    "(df_caption.text.apply(str.split).apply(len)).quantile([.5,.6,.7,.8,.9,.95,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2894687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_sizes(df):\n",
    "    '''\n",
    "    Visualizes the height and width of the images in nr of pixels.\n",
    "    '''\n",
    "    # read image sizes\n",
    "    widths, heights = [], []\n",
    "    for i in range(len(df)):\n",
    "        filename = df.iloc[i]\n",
    "        img = mpimg.imread(image_path + filename)\n",
    "        width, heigth, chanels = np.shape(img)\n",
    "        widths.append(width)\n",
    "        heights.append(heigth)\n",
    "\n",
    "    #create plot\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(widths, heights)\n",
    "    plt.title(\"Image sizes\")\n",
    "    plt.xlabel(\"width of images in pixels\")\n",
    "    plt.xlabel(\"heights of images in pixels\")\n",
    "    plt.show()\n",
    "\n",
    "unique_filenames = df_caption[\"filename\"].drop_duplicates()\n",
    "# plot_image_sizes(unique_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166435ce",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba02b77",
   "metadata": {},
   "source": [
    "### Preprocessing Images\n",
    "\n",
    "Preprocessing the images includes the following transformations: \n",
    "- `ToPILImage` Transformes the input images to a PIL image which provides the python interpreter with editing capabilities using the **P**ython **I**maging **L**ibrary.\n",
    "- `CenterCrop` Crops the images from the center, resulting in a fixed image resolution. Images with less pixels than specified recieve a padding of zeros to fill the gap.\n",
    "- `ToTensor` Trainforms the numpy format to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, ToPILImage, Normalize\n",
    "\n",
    "image_transform = Compose([\n",
    "    ToPILImage(),\n",
    "    CenterCrop((224, 224)), # resnet18 input shape\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701231cb",
   "metadata": {},
   "source": [
    "### Preprocessing Captions\n",
    "\n",
    "In this section, the captions for the images are preprocessed. The captions are originally provided as strings. In a first step they are processed using the `basic_english` tokenizer included in the torchtext library. It performs several operations such as: lowercasing and replacing certain symbols using a pattern dict. We also limit the maximum number of words per caption to 20, since over 95 percent of all captions are within this range. Sentences with less than 20 words are padded using the `<pad>` token. Finally, we mark the beginning `<bos>` and end `<eos>` with the corresponding tokens, giving all captions a fixed length of 22 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define special tokens\n",
    "start_token = \"<bos>\"\n",
    "stop_token = \"<eos>\"\n",
    "unknown_token = \"<unk>\"\n",
    "padding_token = \"<pad>\"\n",
    "\n",
    "# define caption boundaries\n",
    "max_length  = 20\n",
    "\n",
    "# specify tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def preprocess_caption(text):\n",
    "    '''\n",
    "    Tokenizes the captions and applies preprocessing steps.\n",
    "    '''\n",
    "    # tokenize words with torchtext\n",
    "    tokens = tokenizer(text)\n",
    "    # cut list length to max_length\n",
    "    tokens = tokens[:max_length]\n",
    "    #pad to short sentences\n",
    "    tokens = tokens + [padding_token] * (max_length - len(tokens))\n",
    "    # add start and end token\n",
    "    tokens = [start_token] + tokens + [stop_token]\n",
    "    return tokens\n",
    "\n",
    "df_caption[\"text_tokens\"] = df_caption[\"text\"].apply(preprocess_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992595e",
   "metadata": {},
   "source": [
    "### Define Embedding \n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from torchtext.vocab import Vocab, GloVe\n",
    "\n",
    "#define embeding method\n",
    "vectors = \"glove.6B.100d\"\n",
    "\n",
    "# define minimal required occurence of words\n",
    "min_word_count = 3\n",
    "\n",
    "# count vocabulary\n",
    "vocab_count = Counter()\n",
    "for capiton in df_caption[\"text_tokens\"]:\n",
    "    vocab_count.update(capiton)\n",
    "sorted_by_freq_tuples = sorted(vocab_count.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "# define vocabulary\n",
    "vocab = Vocab(\n",
    "    vocab_count,\n",
    "    vectors=vectors,  \n",
    "    min_freq=min_word_count, \n",
    "    specials=((start_token, stop_token, unknown_token, padding_token)))\n",
    "\n",
    "# comparison between vocabs\n",
    "glove = GloVe(name='6B', dim=100)\n",
    "print(\"GloVe vocab:\", glove.vectors.size())\n",
    "\n",
    "print(\"Reduced vocab:\", vocab.vectors.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27eb402",
   "metadata": {},
   "source": [
    "**Description:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7cfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_tokens(text):\n",
    "    '''\n",
    "    Encodes the tokens from string to integer using our vocabulary\n",
    "    '''\n",
    "    return [vocab.stoi[word] for word in text]\n",
    "\n",
    "def inverse_embed_tokens(text):\n",
    "    '''\n",
    "    Encodes the tokens from integer to string using our vocabulary\n",
    "    '''\n",
    "    return [vocab.itos[word] for word in text]\n",
    "\n",
    "text_list = [\"<bos>\", \"Simon\", \"is\", \"in\", \"this\", \"picture\", \":)\", \"<eos>\"]\n",
    "\n",
    "embedded_text = embed_tokens(text_list)\n",
    "print(\"Encoding:\", embedded_text)\n",
    "reconstructed_text = inverse_embed_tokens(embedded_text)\n",
    "print(\"Inverse Encoding:\", reconstructed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caption[\"text_encoded\"] = df_caption[\"text_tokens\"].apply(embed_tokens)\n",
    "df_caption[[\"text\", \"text_tokens\", \"text_encoded\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa0415",
   "metadata": {},
   "source": [
    "**Description:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23759ab6",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a565e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, test_files = train_test_split(unique_filenames, test_size=0.2)\n",
    "df_train = df_caption.loc[ df_caption[\"filename\"].isin( list(train_files) )]\n",
    "df_test = df_caption.loc[ df_caption[\"filename\"].isin( list(test_files) )]\n",
    "\n",
    "train_img_labels = set(df_train[\"filename\"])\n",
    "test_img_labels = set(df_test[\"filename\"])\n",
    "print(\"Proportion of train set:\", len(train_img_labels) / (len(train_img_labels) + len(test_img_labels)))\n",
    "print(\"Proportion of test set:\", len(test_img_labels) / (len(train_img_labels) + len(test_img_labels)))\n",
    "print(\"Overlapping labels of train and test set:\", sum([label in train_img_labels for label in test_img_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89216c91",
   "metadata": {},
   "source": [
    "### Create train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acedd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr8kDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, image_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pandas DataFrame): contains the filenames and the captions of the pictures\n",
    "            transform (callable, optional): Optional transform to apply on the images\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df_row = self.df.iloc[idx, :]\n",
    "        image = mpimg.imread(image_path + df_row['filename'])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        caption = torch.from_numpy(np.array(df_row['text_encoded']))\n",
    "        length = torch.from_numpy(np.array(len(df_row['text_encoded'])))\n",
    "        return image, caption, length\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5dd3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Flickr8kDataset(df_train, image_path, transform=image_transform)\n",
    "test_set = Flickr8kDataset(df_test, image_path, transform=image_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99803ab7",
   "metadata": {},
   "source": [
    "### Define the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d161d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed \n",
    "torch.manual_seed(42)\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_set, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_set, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = iter(train_dataloader)\n",
    "samples, labels, length = example_batch.next()\n",
    "np.shape(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21d887",
   "metadata": {},
   "source": [
    "### Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, train_cnn=False, dropout=0):\n",
    "        '''\n",
    "        Args:\n",
    "            embed_size (int)\n",
    "            train_cnn (bool) if true trains the complete network\n",
    "            dropout (float): dropout ratio after the last layer\n",
    "        '''\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_cnn = train_cnn\n",
    "        self.cnn_model = models.resnet18(pretrained=True)\n",
    "        self.cnn_model.fc = nn.Linear(self.cnn_model.fc.in_features, embed_size) # resize outout shape\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.cnn_model(images)\n",
    "\n",
    "        # specify if the complete network should be trained or only the last one\n",
    "        for name, param in self.cnn_model.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = self.train_cnn\n",
    "        return self.dropout(self.relu(features))\n",
    "\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout=0):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size) # ????????????????\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, \n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            # batch_first=True\n",
    "                            )\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # print(\"captions shape\", np.shape(captions))\n",
    "        # embedding of captions\n",
    "        embeddings = self.dropout(self.embedding(captions))\n",
    "        # print(\"Embedded captions shape\", np.shape(embeddings))\n",
    "        # print(\"features shape\", np.shape(features))\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        \n",
    "        hiddens, _ = self.lstm(embeddings) #(1, batch size, len_embedding)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "class CNNtoRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout=0):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size=embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, images, capitons):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, capitons)\n",
    "        return outputs\n",
    "\n",
    "    def caption_images(self, image, vocabulary, max_length = 30):\n",
    "        '''Creates a caption for a single image'''\n",
    "        caption_result = []\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hidden, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hidden.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "\n",
    "                caption_result.append(predicted.item())\n",
    "                x = self.decoderRNN.embedding(predicted).unsqueeze(0)\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<eos>\":\n",
    "                    break\n",
    "            return [vocabulary.itos[idx] for idx in caption_result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2469c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderCNN(nn.Module):\n",
    "#     def __init__(self, embed_size):\n",
    "#         \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "#         super(EncoderCNN, self).__init__()\n",
    "#         resnet = models.resnet152(pretrained=True)\n",
    "#         modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "#         self.resnet = nn.Sequential(*modules)\n",
    "#         self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "#         self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "#     def forward(self, images):\n",
    "#         \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "#         with torch.no_grad():\n",
    "#             features = self.resnet(images)\n",
    "#         features = features.reshape(features.size(0), -1)\n",
    "#         features = self.bn(self.linear(features))\n",
    "#         return features\n",
    "\n",
    "\n",
    "# class DecoderRNN(nn.Module):\n",
    "#     def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "#         \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "#         super(DecoderRNN, self).__init__()\n",
    "#         self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "#         self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "#         self.max_seg_length = max_seq_length\n",
    "        \n",
    "#     def forward(self, features, captions, lengths):\n",
    "#         \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "#         embeddings = self.embed(captions)\n",
    "#         print(\"captions shape\", np.shape(captions))\n",
    "#         embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "#         packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "#         hiddens, _ = self.lstm(packed)\n",
    "#         outputs = self.linear(hiddens[0])\n",
    "#         return outputs\n",
    "    \n",
    "#     def sample(self, features, states=None):\n",
    "#         \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "#         sampled_ids = []\n",
    "#         inputs = features.unsqueeze(1)\n",
    "#         for i in range(self.max_seg_length):\n",
    "#             hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "#             outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "#             _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "#             sampled_ids.append(predicted)\n",
    "#             inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "#             inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "#         sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "#         return sampled_ids\n",
    "\n",
    "\n",
    "def wandb_log(log_dict, e):\n",
    "    wandb.log(log_dict, step=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "wandb.init(project=\"del_mc2_image_cap\", entity=\"Simon\")\n",
    "\n",
    "# hyperparameters\n",
    "embedding_size=256\n",
    "hidden_size = 256\n",
    "vocab_size = vocab.vectors.size()[0]\n",
    "num_layers = 1\n",
    "learning_rate = 0.0003\n",
    "num_epochs=3\n",
    "\n",
    "model = CNNtoRNN(embedding_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss() # ignore_index\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch:\", epoch)\n",
    "    for i, (imgs, captions, lengths) in tqdm(enumerate(train_dataloader)):\n",
    "        step += len(imgs)\n",
    "        # print(np.shape(imgs), np.shape(captions))\n",
    "        imgs = imgs.to(device)\n",
    "        captions = captions.to(device)\n",
    "        # targets = pack_padded_sequence(captions, length, batch_first=True)[0]\n",
    "        targets = pack_padded_sequence(captions, lengths, enforce_sorted=False, batch_first=True)[0]\n",
    "        # print(targets)\n",
    "        outputs = model(imgs, captions[:,:-1])\n",
    "        \n",
    "        # print(np.shape(captions.reshape(-1)))\n",
    "        # # outputs = pack_padded_sequence(outputs, lengths.tolist(), enforce_sorted=False, batch_first=True)[0]\n",
    "        # print(np.shape(outputs.reshape(-1, outputs.shape[2])))\n",
    "        # print(np.shape(targets))\n",
    "        # # loss = criterion(outputs, targets)\n",
    "        loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1).type(torch.long))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        wandb.log({\"train_loss\": loss}, step=step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(outputs[0])):\n",
    "    print(vocab.itos[outputs[0][i].argmax().item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3d952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(outputs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777054fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.caption_images(vocab)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d62d65b6a89b65c382b60555ecadb9b7aea59040937a779dac0402d0fe0f0506"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
