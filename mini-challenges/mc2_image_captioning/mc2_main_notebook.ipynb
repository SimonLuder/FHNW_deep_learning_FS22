{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141c0c5d",
   "metadata": {},
   "source": [
    "# Deep Learning Mini-Challenge 2: Image Captioning\n",
    "\n",
    "**Task description:** The aim of this Mini Challenge is to train an image captioning model. After the training, the model should be able to receive an image and generate a single sentence describing the captured scene. This work is strongly inspired by the paper from Vinyals: *Show and Tell: A Neural Image Caption Generator* (https://arxiv.org/pdf/1411.4555.pdf).\n",
    "\n",
    "**Description of the dataset:** The Flickr8k data set is used for training the models. It consists of 8091 different images with varying resolutions. The images were collected from six different Flickr groups and were manually selected to include a range of scenes and situations. In addition, 5 captions are included in the dataset for each image which results in a total of 40455 captions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import wandb\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torchtext:\", torchtext.__version__)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b09c6",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f322ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(label_path, skip_header=True):\n",
    "    '''\n",
    "    Reads the labels and caption text from the captions.txt file in the specified path\n",
    "    '''\n",
    "    with open(label_path + \"captions.txt\") as f:\n",
    "        if skip_header:\n",
    "            next(f)\n",
    "        lines = f.readlines()\n",
    "        lines = [line.replace(\"\\n\", \"\") for line in lines]\n",
    "        lines = [line.split(\".jpg,\") for line in lines]\n",
    "        filenames = [line[0] + \".jpg\" for line in lines]\n",
    "        text = [line[1] for line in lines]\n",
    "        return(pd.DataFrame([filenames, text], index=([\"filename\", \"text\"])).T)\n",
    "\n",
    "\n",
    "image_path = \"../../data/Flickr8k/Images/\"\n",
    "label_path = \"../../data/Flickr8k/\"\n",
    "\n",
    "\n",
    "df_caption = read_labels(label_path)\n",
    "df_caption.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db09e0b",
   "metadata": {},
   "source": [
    "## Explorative data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600f3003",
   "metadata": {},
   "source": [
    "### Visualization of Images with their corresponding captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_imeages(df, n=3, m=2):\n",
    "    '''\n",
    "    Visualises a number of images with the corresponding captions\n",
    "    '''\n",
    "    fig, axes = plt.subplots(n, m, figsize=(22,14))\n",
    "    unique_files = df_caption.filename.unique()\n",
    "\n",
    "    for i in range(n*m):\n",
    "        filename = unique_files[i]\n",
    "        caption = \"\\n\".join(list(df.loc[df[\"filename\"]==unique_files[i]][\"text\"]))\n",
    "        img = mpimg.imread(image_path + filename)\n",
    "        axes[i//m, i%m].imshow(img)\n",
    "        axes[i//m, i%m].set_title(caption)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_sample_imeages(df_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf98f4f",
   "metadata": {},
   "source": [
    "**Description:** We see the first six images from the dataset with their corresponding captions. The images have varying resolutions. The scenes contain people or animals performing a simple action. The captions in the dataset seem relatively clean at first glance. However, there are case-sensitive differences for individual words. In general, some editing will be necessary for the images and the captions, but the effort will probably not be too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477ead4",
   "metadata": {},
   "source": [
    "### Average caption lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9327258",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.ecdfplot(df_caption.text.apply(str.split).apply(len))\n",
    "plt.title(\"ecdf of nr of words per caption\")\n",
    "plt.xlabel(\"nr of words\")\n",
    "plt.ylabel(\"proportion\")\n",
    "plt.show()\n",
    "\n",
    "(df_caption.text.apply(str.split).apply(len)).quantile([.5,.6,.7,.8,.9,.95,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53984fd5",
   "metadata": {},
   "source": [
    "**Description:** The captions in the data set are a maximum of 38 words long. A word length of 19 would already be sufficient for over 95 percent of the captions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7165d3a",
   "metadata": {},
   "source": [
    "### Image resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2894687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_sizes(df):\n",
    "    '''\n",
    "    Visualizes the height and width of the images in nr of pixels.\n",
    "    '''\n",
    "    # read image sizes\n",
    "    widths, heights = [], []\n",
    "    for i in range(len(df)):\n",
    "        filename = df.iloc[i]\n",
    "        img = mpimg.imread(image_path + filename)\n",
    "        width, heigth, chanels = np.shape(img)\n",
    "        widths.append(width)\n",
    "        heights.append(heigth)\n",
    "\n",
    "    #create plot\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(widths, heights)\n",
    "    plt.title(\"Image sizes\")\n",
    "    plt.xlabel(\"width of images in pixels\")\n",
    "    plt.ylabel(\"heights of images in pixels\")\n",
    "    plt.show()\n",
    "\n",
    "unique_filenames = df_caption[\"filename\"].drop_duplicates()\n",
    "plot_image_sizes(unique_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a654c3",
   "metadata": {},
   "source": [
    "**Descriptions:** The visualisation shows the resolution of the images with their number of pixels in height and width. As already recognised in the visualised samples, the images vary in their resolution. However, a clear maximum boundary of 500 pixels is noticeable in the height and width over all images. For the CNN, it is necessary in our case that all images have the same resolution. For this reason, the images must be processed in a next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166435ce",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba02b77",
   "metadata": {},
   "source": [
    "### Preprocessing Images\n",
    "\n",
    "This section handles the preprocessing of the images, which includes the following transformations: \n",
    "- `ToPILImage` Transformes the input images to a PIL image which provides the python interpreter with editing capabilities using the **P**ython **I**maging **L**ibrary.\n",
    "- `CenterCrop` Crops the images from the center, resulting in a fixed image resolution.  In our case since the maximal resolution is 500 x 500 pixels, the images with less pixels recieve a padding of zeros to fill the gap.\n",
    "- `Resize` Images to a resolution of 244 x 244 pixels which is the minimal input size for the Resnet18. Because we center croped all images to the same size before, a distortion of the images is avoided.\n",
    "- `ToTensor` Trainforms the numpy format to a tensor.\n",
    "- `Normalize` Normalizes the rgb channels of the dataset samples with the average values of the rgb channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, CenterCrop, Resize, ToTensor, ToPILImage, Normalize\n",
    "\n",
    "image_transform = Compose([\n",
    "    ToPILImage(),\n",
    "    CenterCrop((500, 500)),  # padd images\n",
    "    Resize((224, 224)), # resnet18 minimal input shape\n",
    "    ToTensor(),\n",
    "    Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "# image_transform = Compose([\n",
    "#     ToPILImage(),\n",
    "#     CenterCrop((224, 224)), # resnet18 input shape\n",
    "#     ToTensor(),\n",
    "#     Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701231cb",
   "metadata": {},
   "source": [
    "### Preprocessing Captions\n",
    "\n",
    "In this section, the captions for the images are preprocessed. The captions are originally provided as strings. In a first step they are processed using the `basic_english` tokenizer included in the torchtext library. It performs several operations such as: lowercasing and replacing certain symbols using a pattern dict. We also limit the maximum number of words per caption to 20, since over 95 percent of all captions are within this range. Sentences with less than 20 words are padded using the `<pad>` token. Finally, we mark the beginning `<bos>` and end `<eos>` with the corresponding tokens, giving all captions a fixed length of 22 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define special tokens\n",
    "start_token = \"<bos>\"\n",
    "stop_token = \"<eos>\"\n",
    "unknown_token = \"<unk>\"\n",
    "padding_token = \"<pad>\"\n",
    "\n",
    "# define caption boundaries\n",
    "max_length  = 20\n",
    "\n",
    "# specify tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def preprocess_caption(text):\n",
    "    '''\n",
    "    Tokenizes the captions and applies preprocessing steps.\n",
    "    '''\n",
    "    # tokenize words with torchtext\n",
    "    tokens = tokenizer(text)\n",
    "    # cut list length to max_length\n",
    "    tokens = tokens[:max_length]\n",
    "    # add start and end token\n",
    "    tokens = [start_token] + tokens + [stop_token]\n",
    "    len_tokens = len(tokens)\n",
    "    #pad to short sentences\n",
    "    tokens = tokens + [padding_token] * (max_length + 2 - len(tokens))\n",
    "    return tokens\n",
    "\n",
    "def add_caption_lengths(text):\n",
    "    return sum([x != \"<pad>\" for x in text])\n",
    "\n",
    "df_caption[\"text_tokens\"] = df_caption[\"text\"].apply(preprocess_caption)\n",
    "\n",
    "df_caption[\"caption_length\"] = df_caption[\"text_tokens\"].apply(add_caption_lengths) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992595e",
   "metadata": {},
   "source": [
    "### Define Embedding \n",
    "\n",
    "Our network does not use the actual words from the captions in the dataset but takes features from an embedding as a vector representation. The aim of embedding is to transform the high dimensionality of the words into a vector space in a meaningful way. This space is significantly smaller in dimensionality, which makes the training more efficient. \n",
    "On one hand, it is necessary to define the vocabulary. On the other hand, it is required to create an embedding for the individual words with this vocabulary. There are two ways to do this. Firstly, it is possible to train an own embedding. The implementation is not too complicated, but since the embedding must also be trained, the learning curve of the network slows down significantly and can generally prevent from reaching the full potential of the predictions. The second option would be to use pre-trained word embedding vectors. An example of pre-trained word embedding would be by using [GloVe](https://nlp.stanford.edu/projects/glove/). \n",
    "\n",
    "\n",
    "Below we use the torchtext Vocab class to generate a onehot encoded vocabulary from the captions and read out the corresponding vectors from GloVe. In addition, the vocabulary is supplemented with our four keywords \\<bos\\>, \\<eos\\>, \\<pad\\> and \\<unk\\>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from torchtext.vocab import Vocab, GloVe\n",
    "\n",
    "#define embeding method\n",
    "vectors = \"glove.6B.100d\"\n",
    "\n",
    "# define minimal required occurence of words\n",
    "min_word_count = 3\n",
    "\n",
    "# count vocabulary\n",
    "vocab_count = Counter()\n",
    "for capiton in df_caption[\"text_tokens\"]:\n",
    "    vocab_count.update(capiton)\n",
    "sorted_by_freq_tuples = sorted(vocab_count.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "# define vocabulary\n",
    "vocab = Vocab(\n",
    "    vocab_count,\n",
    "    vectors=vectors,  \n",
    "    min_freq=min_word_count, \n",
    "    specials=((start_token, stop_token, unknown_token, padding_token)))\n",
    "\n",
    "# comparison between vocabs\n",
    "glove = GloVe(name='6B', dim=100)\n",
    "print(\"GloVe vocab:\", glove.vectors.size())\n",
    "\n",
    "print(\"Reduced vocab:\", vocab.vectors.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27eb402",
   "metadata": {},
   "source": [
    "**Description:** We see that the raw glove embedding contains a vocabulary of 400,000 tokens. Our reduced vocabulary, on the other hand, has 4094 words, barely 1 percent of that.\n",
    "For these words we now have the corresponding pre-trained embeding `vocab.vectors` which could be passed to the embedding layer in our later model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cbbcf6",
   "metadata": {},
   "source": [
    "#### Encoding of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7cfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(text):\n",
    "    '''\n",
    "    Encodes the tokens from string to integer using our vocabulary\n",
    "    '''\n",
    "    return [vocab.stoi[word] for word in text]\n",
    "\n",
    "def inverse_encode_tokens(text):\n",
    "    '''\n",
    "    Encodes the tokens from integer to string using our vocabulary\n",
    "    '''\n",
    "    return [vocab.itos[word] for word in text]\n",
    "\n",
    "text_list = [\"<bos>\", \"Anton\", \"is\", \"in\", \"this\", \"picture\", \":)\", \"<eos>\"]\n",
    "\n",
    "print(\"Input:\", text_list )\n",
    "embedded_text = encode_tokens(text_list)\n",
    "print(\"Encoding:\", embedded_text)\n",
    "reconstructed_text = inverse_encode_tokens(embedded_text)\n",
    "print(\"Inverse Encoding:\", reconstructed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18830f26",
   "metadata": {},
   "source": [
    "**Description:** We see an example of how the encoding works. The special tokens \\<bos\\> and \\<eos\\> are correctly encoded and decoded again. Since \"Anton\" and \":)\" do not occur in our generated vocabulary, they are encoded with the unknown token \\<unk\\>. So the encoding and decoding works. In a next step, we therefore apply the onehotencoding to the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caption[\"text_encoded\"] = df_caption[\"text_tokens\"].apply(encode_tokens)\n",
    "df_caption[[\"text\", \"text_tokens\", \"text_encoded\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23759ab6",
   "metadata": {},
   "source": [
    "### Train-test split \n",
    "For training, we divide the data set into a training set and a test set with a ratio of 4/1. Thereby it is relevant to make sure that all captions of a picture are in the same subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a565e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, test_files = train_test_split(unique_filenames, test_size=0.2)\n",
    "df_train = df_caption.loc[ df_caption[\"filename\"].isin( list(train_files) )]\n",
    "df_test = df_caption.loc[ df_caption[\"filename\"].isin( list(test_files) )]\n",
    "\n",
    "\n",
    "# save train and test split dataframes as pickles and load if already exists\n",
    "if os.path.exists(\"./train.pickle\") and os.path.exists(\"./test.pickle\"):\n",
    "    with open(\"./train.pickle\", 'rb') as f:\n",
    "        df_train =  pickle.load(f)\n",
    "    with open(\"./test.pickle\", 'rb') as f:\n",
    "        df_test =  pickle.load(f)\n",
    "else:\n",
    "    with open(\"./train.pickle\", 'wb') as f:\n",
    "        pickle.dump(df_train, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(\"./test.pickle\", 'wb') as f:\n",
    "        pickle.dump(df_test, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "train_img_labels = set(df_train[\"filename\"])\n",
    "test_img_labels = set(df_test[\"filename\"])\n",
    "print(\"Proportion of train set:\", len(train_img_labels) / (len(train_img_labels) + len(test_img_labels)))\n",
    "print(\"Proportion of test set:\", len(test_img_labels) / (len(train_img_labels) + len(test_img_labels)))\n",
    "print(\"Overlapping labels of train and test set:\", sum([label in train_img_labels for label in test_img_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89216c91",
   "metadata": {},
   "source": [
    "### Create train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acedd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr8kDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates the dataset structure for training with pytorch\n",
    "    Args:\n",
    "        df (pandas DataFrame): contains the filenames and the captions of the pictures\n",
    "        image_path (str): path to the image folder\n",
    "        transform (callable, optional): Optional transform to apply on the images\n",
    "        preload (bool): if true preloads the dataset to the memory\n",
    "        \"\"\"\n",
    "    def __init__(self, df, image_path, transform=None, preload=False):\n",
    "        \n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.preload = preload\n",
    "\n",
    "        if self.preload:\n",
    "            self.images = []\n",
    "            for filename in tqdm(np.unique(df['filename'])):\n",
    "                image = mpimg.imread(image_path + filename)\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                self.images.append(image)\n",
    "\n",
    "            self.df['image_idx'] = df.groupby('filename').ngroup()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df_row = self.df.iloc[idx, :]\n",
    "\n",
    "        if self.preload:\n",
    "            image_id = self.df.iloc[idx]['image_idx']\n",
    "            image = self.images[image_id]\n",
    "        else:\n",
    "            image = mpimg.imread(image_path + df_row['filename'])\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "        caption = torch.from_numpy(np.array(df_row['text_encoded']))\n",
    "        length = torch.from_numpy(np.array(df_row['caption_length']))\n",
    "        return image, caption, length\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5dd3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Flickr8kDataset(df_train, image_path, transform=image_transform, preload=True)\n",
    "test_set = Flickr8kDataset(df_test, image_path, transform=image_transform, preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99803ab7",
   "metadata": {},
   "source": [
    "### Define the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d161d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed \n",
    "torch.manual_seed(42)\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_set, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_set, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = iter(train_dataloader)\n",
    "samples, labels, length = example_batch.next()\n",
    "np.shape(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05491929",
   "metadata": {},
   "source": [
    "### Model Structure\n",
    "\n",
    "The `EncoderCNN` model consists of two main components: The image is fed into a deep Convolutional Neural Network (CNN). This generates a vector representation, which is extracted from the last hidden layer. The resulting vector is then used as feature input of the `DecoderRNN`, which contains a a Long Short Term Memory (LSTM) to generate the sentence structure.\n",
    "\n",
    "#### CNN\n",
    "For the CNN, we use the pytorch libaray [Restnet18](https://pytorch.org/hub/pytorch_vision_resnet/) model which has been pre-trained on the ImageNet dataset on a image classification task of 1000 classes. In general, it would also be possible to use any other CNN structure for this task. To be able to use the network for our captioning task, the last hidden layer has to be manually modified and trainied using transfer learning to match the feature vectors on the embedding layer. Therefore the output of the linear layer has to match the dimensions of the embedding layer vectors for the subsequent LSTM network.\n",
    "\n",
    "\n",
    "#### LSTM\n",
    "The LSTM is now used to decode the feature vector. While training, it receives as input a `PackedSequence` consisting of the concatenation of the feature vectors and the embedding. This enables the network to recieve inouts of varying lengths. The output of the LSTM is then transformed back into the vocab_size dimension by an additional linear layer. In this way, the linear layer serves as reverse encoding, whereby the output represents the weights for the assignment to the words in our vocabulary.\n",
    "\n",
    "Due to the additional \"packing\" of the labels using pytorchs `pack_padded_sequence` method, we gain the advantage that the \\<pad\\> tokens are not included in the cost function when calculating the crossentropy loss.\n",
    "\n",
    "During training, the LSTM receives as inputs the tokens that were generated on the original captions. The output of the LSTM is ignored in this stage. If instead the generated output from the last iteration were used as input, the predictions of the LSTM in the later iterations of a sequence would be very strongly dependent on the previously generated output. The network would train on assumptions that are often incorrect, especially at the beginning of the training, and would therefore slow down the training of the network.\n",
    "\n",
    "When predicting with the trained network, no captions are included, so the highest probability token from the output of the previous iteration is used as input for the next iteration of the LSTM.\n",
    "\n",
    "#### Combination\n",
    "\n",
    "Both model classes are integrated into a single `CNNtoRNN` class. This is primarily for structural reasons and allows to call the functionalities of both models in a combined class structure. Additionally, the function for captioning a single image is integrated here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21d887",
   "metadata": {},
   "source": [
    "### Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    '''\n",
    "    The Encoder class\n",
    "    Args:\n",
    "        embed_size (int)\n",
    "        train_cnn (bool) if true trains the complete network\n",
    "    '''\n",
    "    def __init__(self, embed_size, train_cnn=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_cnn = train_cnn\n",
    "        self.cnn_model = models.resnet18(pretrained=True)\n",
    "        self.cnn_model.fc = nn.Linear(self.cnn_model.fc.in_features, embed_size) # resize outout shape\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.cnn_model(images)\n",
    "\n",
    "        # specify if the complete network should be trained or only the last one\n",
    "        for name, param in self.cnn_model.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = self.train_cnn\n",
    "        return self.bn(self.relu(features))\n",
    "\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    '''\n",
    "    The decoder class\n",
    "    Args:\n",
    "        embed_size (int): size of the embeddings\n",
    "        hidden_size (int): equal to embed size\n",
    "        vocab_size (int): equals the number of words in the vocab\n",
    "        num_layers (int): number of layers in the lstm\n",
    "        dropout (float): dropout ratio for the embeddings\n",
    "        pretrained_emb (bool): if true uses the pretrained glove embedding vectors\n",
    "    '''\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout=0, pretrained_emb=False):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        if pretrained_emb:\n",
    "            self.embedding = nn.Embedding.from_pretrained(vocab.vectors, freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size) # ????????????????\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embed_size, \n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            # batch_first=True\n",
    "                            )\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        # print(\"captions shape\", np.shape(captions))\n",
    "        # embedding of captions\n",
    "        embeddings = self.dropout(self.embedding(captions))\n",
    "        # print(\"Embedded captions shape\", np.shape(embeddings))\n",
    "        # print(\"features shape\", np.shape(features))\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, enforce_sorted=False, batch_first=True)\n",
    "\n",
    "        output_packed, hidden = self.lstm(packed) #(1, batch size, len_embedding)\n",
    "        output_padded, output_lengths = pad_packed_sequence(output_packed, batch_first=True)\n",
    "        \n",
    "        outputs = self.linear(output_padded)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "class CNNtoRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout=0, pretrained_emb=False):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size=embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, images, capitons, lengths):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, capitons, lengths)\n",
    "        return outputs\n",
    "\n",
    "    def caption_images(self, image, vocabulary, max_length = 30):\n",
    "        '''Creates a caption for a single image'''\n",
    "        caption_result = []\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "       \n",
    "            for _ in range(max_length):\n",
    "                hidden, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hidden.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "\n",
    "                caption_result.append(predicted.item())\n",
    "                x = self.decoderRNN.embedding(predicted).unsqueeze(0)\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<eos>\":\n",
    "                    break\n",
    "            return \"\".join([vocabulary.itos[idx] + \" \" for idx in caption_result])\n",
    "\n",
    "\n",
    "\n",
    "def caption_image(model, path=image_path, filename=None, transform=image_transform):\n",
    "    '''\n",
    "    Creates a caption for a single image\n",
    "    Args:\n",
    "        model (CNNtoRNN): model class\n",
    "        path (str): path to image folder\n",
    "        filename (str): name of image, if None a random image is selected\n",
    "        transform (torchvision.transforms.Compose): the transformations of the image before feeding into the model\n",
    "    '''\n",
    "    model.eval()\n",
    "    if not filename:\n",
    "        filename = random.choice(os.listdir(path))\n",
    "    image = mpimg.imread(path + filename)\n",
    "    if transform is not None:\n",
    "        image_tensor = transform(image)\n",
    "    with torch.no_grad():\n",
    "        image_tensor = (image_tensor[None, ...]).to(device)\n",
    "        return filename, image, model.caption_images(image_tensor, vocab)  \n",
    "\n",
    "def func_save_model(model, path, name):\n",
    "        '''\n",
    "        Saves the model as state dict.\n",
    "        Args:\n",
    "            model (CNNtoRNN): the model class\n",
    "            path (str): path where to save the model\n",
    "            name of the document\n",
    "        '''\n",
    "        filename = \"{}.pt\".format(name)\n",
    "        print(filename)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(model, path + filename)\n",
    "\n",
    "def func_save_log(log_dict, name):\n",
    "    '''\n",
    "    Saves the logging dictionary as pickle object\n",
    "    Args: \n",
    "        log_dict (dict): contains the logged metricts from the training\n",
    "        name (str): the name of the pickle file\n",
    "    '''\n",
    "    with open('{}.pickle'.format(name), 'wb') as f:\n",
    "        pickle.dump(log_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def func_open_log(name):\n",
    "    '''\n",
    "    Opens the logging dictionary from a pickle object\n",
    "    Args: \n",
    "        name (str): the name of the pickle file\n",
    "    Returns: \n",
    "        log_dict (dict): contains the logged metricts from the training\n",
    "    '''\n",
    "    with open('{}.pickle'.format(name), 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "run_cell = False\n",
    "# wandb\n",
    "use_wandb=False\n",
    "# model loading & saving\n",
    "load_model = True\n",
    "save_model = True\n",
    "model_path = \"./saved_models/\"\n",
    "model_name = \"pretrained_embedding\"\n",
    "# training hyperparameters\n",
    "pretrained_emb = True\n",
    "embed_size=100\n",
    "hidden_size = 100\n",
    "vocab_size = vocab.vectors.size()[0]\n",
    "num_layers = 1\n",
    "learning_rate = 0.003\n",
    "num_epochs=40\n",
    "# BLEU evaluation\n",
    "include_bleu_test_score = True\n",
    "n_samples = 64 # nr of random samples for BLEU score\n",
    "\n",
    "\n",
    "if run_cell:\n",
    "    # define training modules\n",
    "    if load_model:\n",
    "        log_dict = func_open_log(model_name)\n",
    "        step = log_dict[max(int(k) for k in log_dict.keys())][\"step\"]\n",
    "        model = torch.load(model_path + \"{}_e{}.pt\".format(model_name, len(log_dict)), map_location=device)\n",
    "    else: \n",
    "        log_dict = dict()\n",
    "        step = 0\n",
    "        model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers, pretrained_emb).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss() # ignore_index\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    if use_wandb:\n",
    "        wandb.init(reinit=True, project=\"del_mc2\", entity=\"simonluder\")\n",
    "\n",
    "\n",
    "    for epoch in range(len(log_dict)+1,len(log_dict)+1+num_epochs):\n",
    "        print(\"epoch:\", epoch)\n",
    "        cumloss = 0\n",
    "\n",
    "        model.train()\n",
    "        for i, (imgs, captions, lengths) in tqdm(enumerate(train_dataloader)):\n",
    "            \n",
    "            step += len(imgs)\n",
    "            # print(np.shape(imgs), np.shape(captions))\n",
    "            imgs = imgs.to(device)\n",
    "            captions = captions.to(device)\n",
    "            # targets = pack_padded_sequence(captions, length, batch_first=True)[0]\n",
    "\n",
    "            packed_captions = pack_padded_sequence(captions, lengths, enforce_sorted=False, batch_first=True)[0]\n",
    "            # print(targets)\n",
    "            outputs = model(imgs, captions[:,:-1], lengths)\n",
    "            packed_outputs = pack_padded_sequence(outputs, lengths, enforce_sorted=False, batch_first=True)[0]\n",
    "            # print(\"packed_captions\", np.shape(packed_captions))\n",
    "            # print(\"packed_outputs\", np.shape(packed_outputs))\n",
    "            \n",
    "            # print(np.shape(captions.reshape(-1)))\n",
    "            # # outputs = pack_padded_sequence(outputs, lengths.tolist(), enforce_sorted=False, batch_first=True)[0]\n",
    "            # print(np.shape(outputs.reshape(-1, outputs.shape[2])))\n",
    "            # print(np.shape(targets))\n",
    "            # # loss = criterion(outputs, targets)\n",
    "            # loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1).type(torch.long))\n",
    "            loss = criterion(packed_outputs, packed_captions.type(torch.long))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cumloss += loss.item()\n",
    "\n",
    "            if i % 10 == 0 and use_wandb:\n",
    "                wandb.log({\n",
    "                    \"model\":model_name,\n",
    "                    \"train_loss\": loss,\n",
    "                    \"epoch\":epoch+1, \n",
    "                    }, step=step)\n",
    "\n",
    "        model.eval()   \n",
    "        # calculate BLEU\n",
    "        if include_bleu_test_score:\n",
    "            mean_bleu = 0\n",
    "            for j, file in enumerate(random.sample(list(set(df_test[\"filename\"])), n_samples)):\n",
    "                (file, _, caption) = caption_image(model)\n",
    "                train_captions = df_caption.loc[df_caption[\"filename\"]==file][\"text\"].apply(str.split).to_list()\n",
    "                mean_bleu += sentence_bleu(train_captions, caption.split()[1:-1])\n",
    "            mean_bleu /= n_samples\n",
    "\n",
    "            \n",
    "        log_dict[epoch] = {\n",
    "            \"step\":step, \n",
    "            \"train_loss\":cumloss,\n",
    "            \"test_bleu\":mean_bleu\n",
    "            }\n",
    "\n",
    "        # save the model at the end of the epoch\n",
    "        func_save_model(model, model_path, \"{}_e{}\".format(model_name, epoch))\n",
    "        func_save_log(log_dict, model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b9089",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9a2b2",
   "metadata": {},
   "source": [
    "### Visualizing Loss of the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = func_open_log(model_name)\n",
    "test_bleu = []\n",
    "epochs = []\n",
    "train_losses = []\n",
    "\n",
    "for i in range(1, len(log_dict)+1):\n",
    "    test_bleu.append(log_dict[i][\"test_bleu\"])\n",
    "    epochs.append(i)\n",
    "    train_losses.append(log_dict[i][\"train_loss\"])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Cumulative train loss over epochs\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"cumulative loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509bcf0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./saved_models/\"\n",
    "model_name = \"untrained_embedding\"\n",
    "epochs_trained = 250\n",
    "\n",
    "def eval_caption(model, filename=None):\n",
    "    (file, image, caption) = caption_image(model, filename=filename)\n",
    "\n",
    "    caption.split()[1:-1]\n",
    "    train_captions = df_caption.loc[df_caption[\"filename\"]==file][\"text\"].apply(str.split).to_list()\n",
    "\n",
    "    bleu =  sentence_bleu(train_captions, caption.split()[1:-1], weights=(1/2, 1/2))\n",
    "    return file, bleu\n",
    "\n",
    "model = torch.load(model_path + \"{}_e{}.pt\".format(model_name, epochs_trained), map_location=device)\n",
    "df_bleu_scores = pd.DataFrame(columns=[\"filename\", \"bleu_score\", \"set\"])\n",
    "\n",
    "for f in tqdm(set(df_test[\"filename\"])):\n",
    "    (file, bleu) = (eval_caption(model, filename=f))\n",
    "    df_bleu_scores.loc[len(df_bleu_scores.index)] = [file, bleu, \"test\"]\n",
    "\n",
    "for f in tqdm(set(df_train[\"filename\"])):\n",
    "    (file, bleu) = (eval_caption(model, filename=f))\n",
    "    df_bleu_scores.loc[len(df_bleu_scores.index)] = [file, bleu, \"train\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d3a692",
   "metadata": {},
   "source": [
    "### Comparison of bleu scores for train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97428ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bleu_distribution(scores, title):\n",
    "    '''Creates a histogramm of the bleu scores'''\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(scores, bins=25)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"BLEU score\")\n",
    "    plt.ylabel(\"nr of samples\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061368cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bleu_distribution(df_bleu_scores.loc[df_bleu_scores[\"set\"]==\"train\"][\"bleu_score\"], title = \"Distribution of BLEU scores in train set\" )\n",
    "print(\"BLEU scores quantiles for train set:\\n\", df_bleu_scores.loc[df_bleu_scores[\"set\"]==\"train\"][\"bleu_score\"].quantile([0,.25,.5,.75,1]).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726deaeb",
   "metadata": {},
   "source": [
    "**Description:**  The histogram shows the distribution of the achieved BLEU score scale for all samples from the training set. We see that the BLEU scores are distributed over the entire value scale with a concentration around 0.33. There are several captions with very good BLEU scores greater than 0.8. Also there are many samples which have achieved a BLEU score of 0. Subsequently, we now want to make a comparison of the BLEU scores for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032429d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bleu_distribution(df_bleu_scores.loc[df_bleu_scores[\"set\"]==\"test\"][\"bleu_score\"], title = \"Distribution of BLEU scores in test set\" )\n",
    "print(\"BLEU scores quantiles for test set:\\n\", df_bleu_scores.loc[df_bleu_scores[\"set\"]==\"test\"][\"bleu_score\"].quantile([0,.25,.5,.75,1]).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd48f6",
   "metadata": {},
   "source": [
    "**Description:**  The histogram shows the distribution of the achieved BLEU score scale for all samples from the test set. We see that there are significantly more images in the dataset that received a BLEU score of 0. Also, there are almost no images in the value range 0.6 upwards. This might be because of several reasons. On the one hand, a minimal deterioration if the average BLEU Score can be expected due to variance in sentence structures. Also, the test set contains words and objects that were not used in the training. Thus, the model is sometimes not able to recognise these objects at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25395ab5",
   "metadata": {},
   "source": [
    "### Visualizing single examples\n",
    "\n",
    "Now we will take a closer look at a handful of examples with very good and very bad BLEU Scores from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bleu_scores_test = df_bleu_scores.loc[df_bleu_scores[\"set\"]==\"test\"]\n",
    "df_bleu_scores_test = df_bleu_scores_test.sort_values(\"bleu_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def sample_presentation(model, filename):\n",
    "    '''Shows an Image with corresponging caption, bleu score and ranking compared to the other samples in the test set'''\n",
    "    (file, image, caption) = caption_image(model, filename = filename)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    print(\"Captioning image: {}\".format(file))  \n",
    "    print(\"Caption sencence:\", caption)\n",
    "    caption.split()[1:-1]\n",
    "    train_captions = df_caption.loc[df_caption[\"filename\"]==file][\"text\"].apply(str.split).to_list()\n",
    "    bleu =  sentence_bleu(train_captions, caption.split()[1:-1], weights=(1/2, 1/2))\n",
    "    print(\"BLEU score:\",bleu)\n",
    "    print(\"Ranking in test set according to BLEU score:\", int(df_bleu_scores_test.loc[df_bleu_scores_test[\"filename\"] == file].index.values), \"of\", len(df_bleu_scores_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49787bf6",
   "metadata": {},
   "source": [
    "#### Good captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"3640020134_367941f5ec.jpg\", \"3148811252_2fa9490a04.jpg\", \"1019604187_d087bf9a5f.jpg\", \"2660008870_b672a4c76a.jpg\", \"3728015645_b43a60258b.jpg\", \"2313230479_13f87c6bf3.jpg\"]\n",
    "for file in files:\n",
    "    sample_presentation(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78ea59",
   "metadata": {},
   "source": [
    "**Description:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b476b7b",
   "metadata": {},
   "source": [
    "#### Bad captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738caff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"3640020134_367941f5ec.jpg\", \"3148811252_2fa9490a04.jpg\", \"1019604187_d087bf9a5f.jpg\", \"2660008870_b672a4c76a.jpg\", \"3728015645_b43a60258b.jpg\", \"2313230479_13f87c6bf3.jpg\"]\n",
    "for file in files:\n",
    "    sample_presentation(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974d3e9",
   "metadata": {},
   "source": [
    "**Description:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f282207",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0d8c3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Zusammenfassend bin ich mit dem erreichten Ergebnis zufrieden. Die Qualität der Captions schwankt von recht ordentlich bis komplett falsch. Ich denke das Modell ist relativ gut in der Lage einzelne Objekte zu erkennen, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7b617",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('neuerversuchscheisshurensohn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2277a8ecc03c93065c8c91f6aed9c249dc8c0ac64547bdfac1733ee87f23bb40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
