{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Mini-Challenge 1: Image Classification with Neuronal Networks\n",
    "\n",
    "**Task description:** The goal of this mini-challenge is the implementation, optimization and evaluation of a neural network model for the classification of an image dataset. The focus is on the training and evaluation of the models. The evaluation is limited to Multi layer Perceptrons (MLP's) and Convolutional neural Networks (CNN's) in combination with different optimization and regularization methods.\n",
    "\n",
    "**Description of the dataset:** For this mini challenge we use the `CIFAR10` dataset, which is a sub-set of the `Tiny images` dataset. The dataset is named after the **C**anadian **I**nstitute **F**or **A**dvanced **R**esearch. It is an image dataset and contains some 60 000 labelled images that belong to one of 10 classes. The images have a resolution of 32x32 pixels and are in RGB format. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import wandb\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"WANDB_SILENT\"] = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device configuration\n",
    "\n",
    "Check for allocated devices that can be used for training of the Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases init configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "entity, project = \"simonluder\", \"del-mc1\"  # set to your entity and project \n",
    "runs = api.runs(entity + \"/\" + project) \n",
    "\n",
    "summary_list, config_list, name_list, history_list = [], [], [], []\n",
    "for i, run in enumerate(runs): \n",
    "    if i % 20 == 0:\n",
    "        print(\"Reading model:\", i)\n",
    "        \n",
    "    # .summary contains the output keys/values for metrics like accuracy.\n",
    "    #  We call ._json_dict to omit large files \n",
    "    summary_list.append(run.summary._json_dict)\n",
    "    \n",
    "    # .history creates a history dataframe of all logged metrics\n",
    "    history_list.append(run.history())\n",
    "\n",
    "    # .config contains the hyperparameters.\n",
    "    #  We remove special values that start with _.\n",
    "    config_list.append(\n",
    "        {k: v for k,v in run.config.items()\n",
    "         if not k.startswith('_')})\n",
    "\n",
    "    # .name is the human-readable name of the run.\n",
    "    name_list.append(run.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list\n",
    "name_list\n",
    "\n",
    "len(summary_list)\n",
    "history_list\n",
    "\n",
    "df_history_summary = pd.DataFrame()\n",
    "df_config_summary = pd.DataFrame()\n",
    "for i, name in enumerate(name_list):\n",
    "    history_list[i][\"name\"] = name\n",
    "    df_history_summary = pd.concat([df_history_summary, history_list[i]])\n",
    "    config_list[i][\"name\"] = name\n",
    "    df_config_summary = df_config_summary.append(config_list[i], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_config_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history_max_test_acc = df_history_summary.groupby(\"name\").max(\"test accuracy\").reset_index()\n",
    "df = pd.merge(df_history_max_test_acc, df_config_summary, left_on=\"name\", right_on=\"name\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best_models(df, variant=None):\n",
    "    if variant:\n",
    "        df = df.loc[df[\"variant\"]==variant]\n",
    "    df = df.loc[df[\"best test accuracy\"]==np.max(df[\"best test accuracy\"])]\n",
    "    print(\"Model created at {} at epoch {} with a test accuracy of {} and test loss of {:.3f}\".format(\n",
    "        df[\"name\"].values[0], \n",
    "        df[\"_step\"].values[0].astype(int), \n",
    "        df[\"test accuracy\"].values[0],\n",
    "        df[\"test loss\"].values[0]))\n",
    "    return df\n",
    "\n",
    "print_best_models(df, variant=\"MLP_SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"_step\"].values[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def create_plot_mlp_sizes(df):\n",
    "    df = df.dropna(axis=0)\n",
    "    df[\"hidden_layer_size\"] = df[\"hidden_layers\"].apply(lambda x: x[0])\n",
    "    df = df[[\"num_hidden\", \"hidden_layer_size\", \"test accuracy\"]].groupby([\"num_hidden\", \"hidden_layer_size\"]).max().reset_index()\n",
    "    df = df.pivot(index='num_hidden', columns='hidden_layer_size', values='test accuracy')\n",
    "   \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z = df.values,\n",
    "        x = df.columns.astype(str),\n",
    "        y = df.index.astype(str),\n",
    "        texttemplate=\"%{z}\",\n",
    "        textfont={\"size\":20}\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text='Best MLP Scores test accuracy scores for varying layers',\n",
    "        xaxis_title=\"hidden layer size\",\n",
    "        yaxis_title=\"number of hidden layers\",\n",
    "        legend_title=\"test accuracy\",)\n",
    "    fig.show()\n",
    "\n",
    "# create_plot_mlp_sizes(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.plot(df_history_summary[\"_step\"], df_history_summary[\"train accuracy\"], hue=df_history_summary[\"name\"])\n",
    "# # df_history_summary\n",
    "\n",
    "# import plotly.express as px\n",
    "\n",
    "# fig = px.line(df_history_summary, x=\"_step\", y=\"test accuracy\", color=\"name\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from plotly.subplots import make_subplots\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# fig = make_subplots(\n",
    "#     rows=2, cols=2,\n",
    "#     subplot_titles=(\"Plot 1\", \"Plot 2\", \"Plot 3\", \"Plot 4\"))\n",
    "\n",
    "# fig.add_trace(px.line(df_history_summary, x=\"_step\", y=\"test accuracy\", color=\"name\"),\n",
    "#               row=1, col=1)\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=[20, 30, 40], y=[50, 60, 70]),\n",
    "#               row=1, col=2)\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=[300, 400, 500], y=[600, 700, 800]),\n",
    "#               row=2, col=1)\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=[4000, 5000, 6000], y=[7000, 8000, 9000]),\n",
    "#               row=2, col=2)\n",
    "\n",
    "# fig.update_layout(height=500, width=700,\n",
    "#                   title_text=\"Multiple Subplots with Titles\")\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Pytorchs torchvision library offers access to a variety of datasets, including CIFAR10. In a first step, the dataset is downloaded and stored in the data folder of our repository. In the future, our dataset will be automatically accessed from there. \n",
    "\n",
    "After that, the dataset gets loaded into the notebook without any transformation to review the data and conduct some analysis. The data is loaded using pytorch's `DataLoader` method. This allows for relatively simple pre-sampling and loading of the data, with the possibility of custom customizing loading order and automatic batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_dataset(normalize=False, path=None):\n",
    "    '''Defines the train and testset\n",
    "    Args:\n",
    "        normalize (bool): if true normalizes the image\n",
    "    Returns:\n",
    "        train_set (torchvision dataset)\n",
    "        test_set (torchvision dataset)\n",
    "    '''\n",
    "    if normalize:\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    else:\n",
    "        transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root=path, \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform)\n",
    "\n",
    "    test_set = torchvision.datasets.CIFAR10(\n",
    "        root=path, \n",
    "        train=False, \n",
    "        download=True, \n",
    "        transform=transform)\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "def define_dataloader(train_set, test_set, batch_size):\n",
    "    '''Defines the dataloader for the train and test set\n",
    "    Args:\n",
    "        train_set (torchvision dataset):\n",
    "        test_set (torchvision dataset):\n",
    "        batch_size (int): defines the number of samples per batch\n",
    "    Returns:\n",
    "        train_loader\n",
    "        test_loader\n",
    "    '''\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        )\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset without normalization\n",
    "train_set, test_set =  define_dataset(normalize=False, path='./../../data')\n",
    "train_loader, test_loader = define_dataloader(train_set, test_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define class labels\n",
    "\n",
    "The CIFAR10 dataset contains ten different classes. In the dataset itself, these are represented by the numerical values 0 to 9. In order to be able to check the individual samples better, we create a dictionary that creates a mapping between the numerical class labels and the real class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n",
    "classes_map = dict(zip(range(len(classes)), classes))\n",
    "classes_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class balance in training and test set\n",
    "\n",
    "We now want to check the frequencies of the individual class labels in the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve train label class count\n",
    "train_label_count = list()\n",
    "for samples, labels in train_loader:\n",
    "    train_label_count.extend(labels.numpy())\n",
    "train_label_count = {classes_map[i]: train_label_count.count(i) for i in set(train_label_count)}\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(list(train_label_count.keys()), list(train_label_count.values()))\n",
    "plt.title(\"Nr of labels per class in train set\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** As can be seen in the visualisation above, the training set has balanced class labels. There are 5000 labels per class for all ten classes which results in 50'000 labels total within the trainig set. Next we want to examine the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve test label class count\n",
    "test_label_count = list()\n",
    "for samples, labels in test_loader:\n",
    "    test_label_count.extend(labels.numpy())\n",
    "test_label_count = {classes_map[i]: test_label_count.count(i) for i in set(test_label_count)}\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(list(test_label_count.keys()), list(test_label_count.values()))\n",
    "plt.title(\"Nr of labels per class in test set\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** The test set also has balanced class labels. There are 1000 samples per class in the test set which means in total, our test set has 10,000 samples. Knowing the size of the train and test set are able to retrieve calculate the split ratio in the CIFAR10 dataset. The resulting split ratio is therefore 5 to 1 between the train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select sample batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = iter(train_loader)\n",
    "samples, labels = example_batch.next()\n",
    "print(\"Sample Dimensions:\", samples.shape, \"\\nLabel Dimension:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** We now see the dimensions of a selected batch from the training set. The sample dimension is 32, 3, 32, 32. These dimensions are composed as follows:\n",
    "**32** samples,\n",
    "**3** colour dimensions (RGB),\n",
    "**32** pixels image height,\n",
    "**32** pixels image width.\n",
    "The label dimension is simply 32 for the labels of the **32** samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 8\n",
    "n = batch_size // m\n",
    "\n",
    "fig, axes = plt.subplots(m, n, figsize=(16,16))\n",
    "for i, image in enumerate(range(batch_size)):\n",
    "    axes[i//n, i%n].imshow(samples[i].permute(1, 2, 0))\n",
    "    axes[i//n, i%n].title.set_text(\"Class: {}\".format(classes[labels[i]]))\n",
    "    axes[i//n, i%n].set_xticks([])\n",
    "    axes[i//n, i%n].set_yticks([])\n",
    "\n",
    "\n",
    "plt.suptitle(\"Visualizing images from training set\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Above we see our randomly drawn training batch. The respective class labels are present in the title. As we can see, the images are not always easy to recognise, even for a human being, due to the low resolution. The biggest advantage of the minimal resolution is the reduction of the features that our model has to take into account, which means that we can use simpler models and correspondingly need less training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the target metrics \n",
    "In this notebook, a set of metrics are used for the evaluation. \n",
    "\n",
    "Since we want to solve a multiclass classification task, the hyperparameter optimization is done primarily by using the cross entropy loss and the balanced accuracy. The score for a perfect prediction would be zero. The initial score of a modell should therefore be higher and start to converge to a lower value. For more information on pytorchs CrossEntropyLoss see: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "Because the dataset has the same proportion of labels within each of its classes, the accuracy equals the balanced accuracy. The accuracy varies in theory between 0 which is the worst possible and 1 which is a perfect prediction. In reality, the accuracy score of our models should start at around 0.1 since we have 10 classes, and it is what could be expected on random prediction of untrained models.Â \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet_2_layer(nn.Module):\n",
    "    '''Multi Layer Perceptron Net class with 2 layers'''\n",
    "    def __init__(self, fc_hidden_1, fc_hidden_2=None, fc_hidden_3=None, activation=F.relu, drop_out=0):\n",
    "        super(MLPNet_2_layer, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=32*32*3, out_features=fc_hidden_1)\n",
    "        self.fc2 = nn.Linear(in_features=fc_hidden_1, out_features=10)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.activation = activation # activation function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size  = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class MLPNet_3_layer(nn.Module):\n",
    "    '''Multi Layer Perceptron Net class with 3 layers'''\n",
    "    def __init__(self, fc_hidden_1, fc_hidden_2, fc_hidden_3=None, activation=F.relu, drop_out=0):\n",
    "        super(MLPNet_3_layer, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=32*32*3, out_features=fc_hidden_1)\n",
    "        self.fc2 = nn.Linear(in_features=fc_hidden_1, out_features=fc_hidden_2)\n",
    "        self.fc3 = nn.Linear(in_features=fc_hidden_2, out_features=10)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.activation = activation # activation function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size  = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class MLPNet_4_layer(nn.Module):\n",
    "    '''Multi Layer Perceptron Net class with 4 layers'''\n",
    "    def __init__(self, fc_hidden_1, fc_hidden_2, fc_hidden_3, activation=F.relu, drop_out=0):\n",
    "        super(MLPNet_4_layer, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=32*32*3, out_features=fc_hidden_1)\n",
    "        self.fc2 = nn.Linear(in_features=fc_hidden_1, out_features=fc_hidden_2)\n",
    "        self.fc3 = nn.Linear(in_features=fc_hidden_2, out_features=fc_hidden_3)\n",
    "        self.fc4 = nn.Linear(in_features=fc_hidden_3, out_features=10)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.activation = activation # activation function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size  = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "class MLPNet_6_layer(nn.Module):\n",
    "    '''Multi Layer Perceptron Net class with 4 layers'''\n",
    "    def __init__(self, fc_hidden_1, fc_hidden_2, fc_hidden_3, fc_hidden_4, fc_hidden_5, activation=F.relu, drop_out=0):\n",
    "        super(MLPNet_6_layer, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=32*32*3, out_features=fc_hidden_1)\n",
    "        self.fc2 = nn.Linear(in_features=fc_hidden_1, out_features=fc_hidden_2)\n",
    "        self.fc3 = nn.Linear(in_features=fc_hidden_2, out_features=fc_hidden_3)\n",
    "        self.fc4 = nn.Linear(in_features=fc_hidden_3, out_features=fc_hidden_4)\n",
    "        self.fc5 = nn.Linear(in_features=fc_hidden_4, out_features=fc_hidden_5)\n",
    "        self.fc6 = nn.Linear(in_features=fc_hidden_5, out_features=10)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.activation = activation # activation function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size  = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset without normalization\n",
    "train_set, test_set =  define_dataset(normalize=True, path='./../../data')\n",
    "train_loader, test_loader = define_dataloader(train_set, test_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \n",
    "    def __init__(self, model, train_loader, test_loader, num_epochs, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.num_epochs = num_epochs\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = {\n",
    "            \"train\":dict(),\n",
    "            \"test\":dict()\n",
    "        }\n",
    "        self.best_log = dict()\n",
    "        self.best_model = None\n",
    "        \n",
    "        \n",
    "    def wandb_log(self, log_dict, e):\n",
    "        wandb.log(log_dict, step=e)\n",
    "\n",
    "\n",
    "    def calculate_accuracy(self, pred, true):\n",
    "        '''Calculates the accuracy for the prediction\n",
    "        Args: \n",
    "            pred: predicted labels\n",
    "            true: true labels\n",
    "        Returns:\n",
    "            accuracy as float\n",
    "        '''\n",
    "        return (pred.argmax(1) == true).type(torch.float).sum().item()\n",
    "    \n",
    "\n",
    "    def train_loop(self, dataloader, verbose):\n",
    "        '''\n",
    "        Loop for a sigle training epoch.\n",
    "        Args:\n",
    "            dataloader: pytorch dataloader\n",
    "        '''\n",
    "        epoch_accuracy = 0\n",
    "        epoch_loss = 0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward propagation\n",
    "            pred = self.model(images)\n",
    "            loss = self.criterion(pred, labels)\n",
    "            \n",
    "            # Backward propagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # calculate metrics\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_accuracy += self.calculate_accuracy(pred, labels)\n",
    "\n",
    "        epoch_loss /= num_batches\n",
    "        epoch_accuracy /= len(dataloader.dataset)\n",
    "        if verbose:\n",
    "            print(f\"train loss:{epoch_loss}, train accuracy:{epoch_accuracy}\")\n",
    "        return {\"train loss\":epoch_loss, \"train accuracy\":epoch_accuracy}\n",
    "\n",
    "\n",
    "    def test_loop(self, dataloader, label=\"train\"):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (images, labels) in enumerate(dataloader):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward propagation\n",
    "                pred = self.model(images)\n",
    "\n",
    "                # calculate metrics\n",
    "                epoch_loss += self.criterion(pred, labels).item()\n",
    "                epoch_accuracy += self.calculate_accuracy(pred, labels)\n",
    "        epoch_loss /= num_batches\n",
    "        epoch_accuracy /= len(dataloader.dataset)\n",
    "        return {\"{} loss\".format(label): epoch_loss, \"{} accuracy\".format(label): epoch_accuracy}\n",
    "\n",
    "\n",
    "    def train(self, verbose=False, include_test_scores=False, early_stopping=False):\n",
    "        # calculate initial model preformance\n",
    "        self.logger[\"train\"][0] = self.test_loop(self.train_loader, label=\"train\")\n",
    "        self.wandb_log(self.logger[\"train\"][0], 0)\n",
    "        if include_test_scores:\n",
    "            self.logger[\"test\"][0] = self.test_loop(self.test_loader, label=\"test\")\n",
    "            self.wandb_log(self.logger[\"test\"][0], 0)\n",
    "            \n",
    "            # save init as best model\n",
    "            self.best_log = self.logger[\"test\"][0]\n",
    "            self.best_model = copy.deepcopy(self.model)\n",
    "            self.logger[\"test\"][0][\"top test accuracy\"] = self.logger[\"test\"][0][\"test accuracy\"]\n",
    "            self.wandb_log({\"top test accuracy\":self.best_log[\"test accuracy\"]}, 0)\n",
    "            \n",
    "\n",
    "        for e in range(self.num_epochs):\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {e+1} ------------------------------\")\n",
    "            \n",
    "            # train model\n",
    "            self.logger[\"train\"][e+1] = self.train_loop(self.train_loader, verbose)\n",
    "            self.wandb_log(self.logger[\"train\"][e+1], e+1)\n",
    "\n",
    "            # calculate model preformance\n",
    "            if include_test_scores:\n",
    "                self.logger[\"test\"][e+1] = self.test_loop(self.test_loader, label=\"test\")\n",
    "                self.wandb_log(self.logger[\"test\"][e+1], e+1)\n",
    "\n",
    "                # secure model with best test score\n",
    "                if self.best_log[\"test accuracy\"] < self.logger[\"test\"][e+1][\"test accuracy\"]:\n",
    "                    self.best_log = self.logger[\"test\"][e+1]\n",
    "                    self.best_model = copy.deepcopy(self.model)\n",
    "                self.logger[\"test\"][e+1][\"top test accuracy\"] = self.best_log[\"test accuracy\"]\n",
    "                self.wandb_log({\"top test accuracy\":self.best_log[\"test accuracy\"]}, e+1)\n",
    "\n",
    "                # stop if test accuracy stops increasing for 20 epochs\n",
    "                if (e >= 20) and early_stopping:\n",
    "                    if self.logger[\"test\"][e+1][\"top test accuracy\"] == self.logger[\"test\"][e-19][\"top test accuracy\"]:\n",
    "                        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 3\n",
    "# model = ConvNet_5(\n",
    "#             conv1_channels=120,\n",
    "#             conv2_channels=120,\n",
    "#             conv3_channels=120,\n",
    "#             conv4_channels=120,\n",
    "#             conv5_channels=120).to(device) # model selection\n",
    "# learning_rate = 0.01        # learning rate for optimizer\n",
    "# l2_weight_decay = 0.0       # l2 regularization for optimizer\n",
    "\n",
    "# trainer = ModelTrainer(\n",
    "#     model=model, \n",
    "#     train_loader=train_loader, \n",
    "#     test_loader=test_loader, \n",
    "#     num_epochs=num_epochs,\n",
    "#     criterion=nn.CrossEntropyLoss(),\n",
    "#     optimizer=torch.optim.SGD(\n",
    "#         model.parameters(), \n",
    "#         lr=learning_rate,\n",
    "#         weight_decay = l2_weight_decay\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# trainer.train(verbose=True, include_test_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"./storage_room\"\n",
    "# filename = \"/test.pth\"\n",
    "\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)\n",
    "\n",
    "# torch.save(trainer.model.state_dict(), path + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP architecture\n",
    "\n",
    "In a first step we want to train a simple MLP with variable number of layers and nodes to see how complex it has to be constructed to be able to learn the training dataset. The number of layers used varies between 2 and 4 (without input layer). Accordingly, the number of hidden layers varies between 1 and 3. The number of nodes also varies between 8 and 200 nodes per layer. For this evaluation, the training per model is stopped after the 100th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %wandb simonluder/del_mc1/reports/Layer_Configurations -h 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** If we compare the scores of the different configurations, we notice that the number of nodes in the first layer correlates strongly with the development of the loss and the accuracy. Each of the best models has more than 150 nodes in its first hidden layer. When examining the number of layers, we see that nets with only 1 hidden layer perform minimally worse on average. However, it is also visible that two hidden layers are already sufficient to learn the training dataset almost perfectly after 100 epochs. \n",
    "\n",
    "Best model with by train loss:\n",
    "- 1 hidden layer: train accuracy: *0.9694* train loss: *0.1351*\n",
    "- 2 hidden layer: train accuracy: *0.9996* train loss: *0.008573*\n",
    "- 3 hidden layer: train accuracy: *0.9997* train loss: *0.004178*\n",
    "\n",
    "\n",
    "Based on these findings, in the further part of this notebook MLP's with two hidden layers will be used. With this configuration we know that the network has enough flexibility to form relevant features for the classification and is able to interpret them accordingly. The fixed number of layers also ensures an easyer interpretability between different downstream models during further hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN architecture\n",
    "\n",
    "The architecture of a Convolutional Neural Network extends the existing structure of our MLP's with Convolutional Layers, followed by a Pooling Layer. The convolutional layer receives as input our 2 dimensional images and the three colour channels (RGB) as a 3d matrix. Then a filter kernel is iterated over the pixels of the matrix window by window. The output for a single pixel is calculated as the scalar product of the iterated window and the filter kernel. The output dimension of the Concolutional Layer can be calculated using the formula below.\n",
    "\n",
    "</br>\n",
    "\n",
    "$$H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor$$\n",
    "\n",
    "$$W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor$$\n",
    "\n",
    "where\n",
    "\n",
    "- $W_{in}$: Width of input\n",
    "- $H_{in}$: Height of input\n",
    "\n",
    "The subsequent pooling layer is used to filter out excess information. Here, again, a window is iterated over the output of the convolutional layer and a pooling action is performed. The most common method is max pooling, but average pooling is also used for deeper neural networks. This reduces the features of our matrices, while it usually does not reduce the accuracy of our network. Due to the feature reduction, our network is not only faster in computation and training, but it is also possible to train deeper networks. Furthermore, this method prevents overfitting, since the individual features are viewed in a generalised manner within the local iterated window. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet_2(nn.Module):\n",
    "    '''A convolutional neural network with 2 convolutional layers'''\n",
    "    def __init__(self, activation=F.relu, kernel_size=5, pool_size=2, conv1_channels = 10, conv2_channels=8, fc_hidden_1=120, fc_hidden_2=84, drop_out=0):\n",
    "        '''Convolutional Net class'''\n",
    "        super(ConvNet_2, self).__init__()   \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=conv1_channels, kernel_size=kernel_size) \n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "        self.conv2 = nn.Conv2d(in_channels=conv1_channels, out_channels=conv2_channels, kernel_size=kernel_size) \n",
    "        self.fc1 = nn.Linear(in_features=conv2_channels*5*5, out_features=fc_hidden_1)   \n",
    "        self.fc2 = nn.Linear(in_features=fc_hidden_1, out_features=fc_hidden_2)\n",
    "        self.fc3 = nn.Linear(in_features=fc_hidden_2, out_features=10)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.activation = activation\n",
    "        self.conv2_channels = conv2_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1(x)))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = x.view(-1, self.conv2_channels*5*5) # flatten tensor\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet_3(nn.Module):\n",
    "    def __init__(self, activation=F.relu, kernel_size=5, pool_size=2, padding=3, conv1_channels = 10, conv2_channels=8, conv3_channels=8, fc_hidden_1=120, fc_hidden_2=84, drop_out=0):\n",
    "        '''A convolutional neural network with 3 convolutional layers'''\n",
    "        super(ConvNet_3, self).__init__()   \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=conv1_channels, kernel_size=kernel_size, padding=padding) \n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "        self.conv2 = nn.Conv2d(in_channels=conv1_channels, out_channels=conv2_channels, kernel_size=kernel_size, padding=padding) \n",
    "        self.conv3 = nn.Conv2d(in_channels=conv2_channels, out_channels=conv3_channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.fc1 = nn.Linear(in_features=conv3_channels*5*5, out_features=fc_hidden_1)   \n",
    "        self.fc2 = nn.Linear(in_features=fc_hidden_1, out_features=fc_hidden_2)\n",
    "        self.fc3 = nn.Linear(in_features=fc_hidden_2, out_features=10)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.activation = activation\n",
    "        self.conv3_channels = conv3_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1(x)))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = self.pool(self.activation(self.conv3(x)))\n",
    "        x = x.view(-1, self.conv3_channels*5*5) # flatten tensor\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class ConvNet_5(nn.Module):\n",
    "    def __init__(self, activation=F.relu, kernel_size=5, pool_size=2, padding=3, conv1_channels = 16, conv2_channels=16, conv3_channels=16, conv4_channels=16, conv5_channels=16, fc_hidden_1=120, fc_hidden_2=84, drop_out=0):\n",
    "        '''A convolutional neural network with 3 convolutional layers'''\n",
    "        super(ConvNet_5, self).__init__()   \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=conv1_channels, kernel_size=kernel_size, padding=padding) \n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "        self.conv2 = nn.Conv2d(in_channels=conv1_channels, out_channels=conv2_channels, kernel_size=kernel_size, padding=padding) \n",
    "        self.conv3 = nn.Conv2d(in_channels=conv2_channels, out_channels=conv3_channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv4 = nn.Conv2d(in_channels=conv3_channels, out_channels=conv4_channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv5 = nn.Conv2d(in_channels=conv4_channels, out_channels=conv5_channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.fc1 = nn.Linear(in_features=conv5_channels*2*2, out_features=fc_hidden_1)   \n",
    "        self.fc2 = nn.Linear(in_features=fc_hidden_1, out_features=fc_hidden_2)\n",
    "        self.fc3 = nn.Linear(in_features=fc_hidden_2, out_features=10)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.activation = activation\n",
    "        self.conv5_channels = conv5_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1(x)))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = self.pool(self.activation(self.conv3(x)))\n",
    "        x = self.pool(self.activation(self.conv4(x)))\n",
    "        x = self.pool(self.activation(self.conv5(x)))\n",
    "        x = x.view(-1, self.conv5_channels*2*2) # flatten tensor\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvNet_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "In this section, different model constelations are examined for their performance. The evaluation is done by using Baesiean Search. The optimization is done with respect to the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path, model, name):\n",
    "        '''\n",
    "        Saves the model as state dict.\n",
    "        Args:\n",
    "            path (str): path where to save the model\n",
    "        '''\n",
    "        filename = \"/{}.pt\".format(name)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(model, path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_2(config=None):\n",
    "    '''Trains and logs a multilayer perceptron'''\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        \n",
    "        # define model\n",
    "        model = MLPNet_2_layer(\n",
    "            fc_hidden_1=config[\"hidden_layers\"][0],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"]))\n",
    "        \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "\n",
    "\n",
    "def train_mlp_3(config=None):\n",
    "    '''Trains and logs a multilayer perceptron'''\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        \n",
    "        # define model\n",
    "        model = MLPNet_3_layer(\n",
    "            fc_hidden_1=config[\"hidden_layers\"][0],\n",
    "            fc_hidden_2=config[\"hidden_layers\"][1],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"]))\n",
    "        \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "\n",
    "\n",
    "def train_mlp_4(config=None):\n",
    "    '''Trains and logs a multilayer perceptron'''\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        \n",
    "        # define model\n",
    "        model = MLPNet_4_layer(\n",
    "            fc_hidden_1=config[\"hidden_layers\"][0],\n",
    "            fc_hidden_2=config[\"hidden_layers\"][1],\n",
    "            fc_hidden_3=config[\"hidden_layers\"][2],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"]))\n",
    "        \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "            \n",
    "def train_mlp_6(config=None):\n",
    "    '''Trains and logs a multilayer perceptron'''\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        \n",
    "        # define model\n",
    "        model = MLPNet_6_layer(\n",
    "            fc_hidden_1=config[\"hidden_layers\"][0],\n",
    "            fc_hidden_2=config[\"hidden_layers\"][1],\n",
    "            fc_hidden_3=config[\"hidden_layers\"][2],\n",
    "            fc_hidden_4=config[\"hidden_layers\"][3],\n",
    "            fc_hidden_5=config[\"hidden_layers\"][4],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"]))\n",
    "        \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "            \n",
    "def train_cnn_2(config=None):\n",
    "    '''Trains and logs a convolutional neuronal network'''\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        \n",
    "        # define model\n",
    "        model = ConvNet_2(\n",
    "            kernel_size=config[\"kernel_size\"],\n",
    "            pool_size=config[\"pool_size\"],\n",
    "            conv1_channels=config[\"conv_layers\"][0],\n",
    "            conv2_channels=config[\"conv_layers\"][1],\n",
    "            fc_hidden_1=config[\"hidden_layers\"][0],\n",
    "            fc_hidden_2=config[\"hidden_layers\"][1],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"]))\n",
    "        \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "            \n",
    "def train_cnn_3(config=None):\n",
    "    '''Trains and logs a convolutional neuronal network'''\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        \n",
    "        # define model\n",
    "        model = ConvNet_3(\n",
    "            kernel_size=config[\"kernel_size\"],\n",
    "            pool_size=config[\"pool_size\"],\n",
    "            conv1_channels=config[\"conv_layers\"][0],\n",
    "            conv2_channels=config[\"conv_layers\"][1],\n",
    "            conv3_channels=config[\"conv_layers\"][2],\n",
    "            fc_hidden_1=config[\"hidden_layers\"][0],\n",
    "            fc_hidden_2=config[\"hidden_layers\"][1],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"]))\n",
    "        \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "            \n",
    "def train_cnn_5(config=None):\n",
    "    '''Trains and logs a convolutional neuronal network'''\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        \n",
    "        # define model\n",
    "        model = ConvNet_5(\n",
    "            kernel_size=config[\"kernel_size\"],\n",
    "            pool_size=config[\"pool_size\"],\n",
    "            conv1_channels=config[\"conv_layers\"][0],\n",
    "            conv2_channels=config[\"conv_layers\"][1],\n",
    "            conv3_channels=config[\"conv_layers\"][2],\n",
    "            conv4_channels=config[\"conv_layers\"][3],\n",
    "            conv5_channels=config[\"conv_layers\"][4],\n",
    "            fc_hidden_1=config[\"hidden_layers\"][0],\n",
    "            fc_hidden_2=config[\"hidden_layers\"][1],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"]))\n",
    "        \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with SGD (without REG, without BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "min_acc = 0.5\n",
    "create_sweep = True #create new sweeps\n",
    "\n",
    "# create initial sweep configuration\n",
    "mlp_sweep_configuration = {\n",
    "    \"name\": \"MLP_sweep\",\n",
    "    \"metric\": {\"name\": \"test accuracy\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\n",
    "        \"variant\": {\n",
    "            \"value\": \"MLP_SGD\"\n",
    "        },\n",
    "        \"num_hidden\": {\n",
    "            \"value\": 1\n",
    "        },\n",
    "        \"hidden_layers\": {\n",
    "            \"values\":[(32,), (64,), (128,), (256,)]\n",
    "        },\n",
    "        \"num_epochs\": {\n",
    "            \"value\": num_epochs\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.2, 0.1, 0.05, 0.02]\n",
    "        },\n",
    "        \"l2_weight_decay\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "        \"drop_out\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# run 2 layer mlp\n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(mlp_sweep_configuration, project=\"del-mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train_mlp_2)\n",
    "\n",
    "    \n",
    "# update sweep configuration for 3 layers mlp\n",
    "mlp_sweep_configuration[\"parameters\"].update({\n",
    "    \"num_hidden\": {\"value\": 2},\n",
    "    \"hidden_layers\": {\"values\":[(32,32), (64,64), (128,128), (256,256)]}})\n",
    "# run 3 layer mlp\n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(mlp_sweep_configuration, project=\"del-mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train_mlp_3)\n",
    "\n",
    "    \n",
    "# update sweep configuration for 4 layers mlp\n",
    "mlp_sweep_configuration[\"parameters\"].update({\n",
    "    \"num_hidden\": {\"value\": 3},\n",
    "    \"hidden_layers\": {\"values\":[(32,32,32), (64,64,64), (128,128,128), (256,256,256)]}})\n",
    "# run 4 layer mlp\n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(mlp_sweep_configuration, project=\"del-mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train_mlp_4)\n",
    "\n",
    "    \n",
    "# update sweep configuration for 6 layers mlp\n",
    "mlp_sweep_configuration[\"parameters\"].update({\n",
    "    \"num_hidden\": {\"value\": 5},\n",
    "    \"hidden_layers\": {\"values\":[(32,32,32,32,32), (64,64,64,64,64), (128,128,128,128,128), (256,256,256,256,256)]}}) \n",
    "# run 6 layer mlp\n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(mlp_sweep_configuration, project=\"del-mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train_mlp_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "min_acc = 0.75\n",
    "create_sweep = True #create new sweeps\n",
    "\n",
    "# create initial sweep configuration\n",
    "cnn_sweep_configuration = {\n",
    "    \"name\": \"CNN_sweep\",\n",
    "    \"metric\": {\"name\": \"test accuracy\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\n",
    "        \"variant\": {\n",
    "            \"value\": \"CNN_SGD\"\n",
    "        },\n",
    "        \"num_conv\": {\n",
    "            \"value\": 2\n",
    "        },\n",
    "        \"conv_layers\": {\n",
    "            \"values\": [(32, 32),(64, 64),(128,128),(256,256)]\n",
    "        },\n",
    "        \"num_hidden\": {\n",
    "            \"value\": 2\n",
    "        },\n",
    "        \"hidden_layers\": {\n",
    "            \"values\": [(128, 64),]\n",
    "        },\n",
    "        \"num_epochs\": {\n",
    "            \"value\": num_epochs\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.2, 0.1, 0.05, 0.02]\n",
    "        },\n",
    "        \"l2_weight_decay\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "        \"drop_out\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "        \"kernel_size\": {\n",
    "            \"value\": 5\n",
    "        },\n",
    "        \"pool_size\": {\n",
    "            \"value\": 2\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# run 2 cnn 2 mlp layer\n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(cnn_sweep_configuration, project=\"del-mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train_cnn_2)\n",
    "\n",
    "    \n",
    "# update sweep configuration for 3 layers cnn\n",
    "cnn_sweep_configuration[\"parameters\"].update({\n",
    "    \"num_conv\": {\"value\": 3},\n",
    "    \"conv_layers\": {\"values\":[(32,32,32), (64,64,64), (128,128,128), (256,256,256)]}})\n",
    "# run 3 cnn 2 mlp layer\n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(cnn_sweep_configuration, project=\"del-mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train_cnn_3)\n",
    "\n",
    "    \n",
    "# update sweep configuration for 5 layers cnn\n",
    "cnn_sweep_configuration[\"parameters\"].update({\n",
    "    \"num_conv\": {\"value\": 5},\n",
    "    \"conv_layers\": {\"values\":[(32,32,32,32,32), (64,64,64,64,64), (128,128,128,128,128), (256,256,256,256,256)]}})\n",
    "# run 5 cnn 2 mlp layer\n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(cnn_sweep_configuration, project=\"del-mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train_cnn_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model:** 2022-05-14-13-33-02 **Test Accurcacy:** 0.7613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%wandb simonluder/del_mc1/reports/Layer_Configurations -h 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusion of Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Regularization\n",
    "\n",
    "The Ridge regression adds the âsquared magnitudeâ of all weights as penalty a term to the loss function. It is relevant how strongly the regularization is weighted in the calculation of the cost function. The strength of the regularization can be set via the hyperparameter *lambda*: $\\lambda$. If lambda is 0, then the regularization is not included in the optimization, which corresponds to the OLS variant used before. The higher lambda is chosen, the more the individual weights are decreased. L2 regularization therefore disperse the error terms in all the weights which leads to more accurate customized final models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_runs = 5\n",
    "min_acc = 0.7\n",
    "\n",
    "create_sweep = False #create new sweeps\n",
    "\n",
    "\n",
    "mlp_sweep_configuration = {\n",
    "    \"name\": \"MLP_sweep\",\n",
    "    \"metric\": {\"name\": \"test accuracy\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"name\": {\n",
    "            \"value\": \"MLP_SGD_L2\"\n",
    "        },\n",
    "        \"fc_hidden_1\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":200,\n",
    "            \"min\":50\n",
    "        },\n",
    "        \"fc_hidden_2\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":180,\n",
    "            \"min\":50\n",
    "        },\n",
    "        \"num_epochs\": {\n",
    "            \"value\": num_epochs\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.1, 0.05]\n",
    "        },\n",
    "        \"l2_weight_decay\": {\n",
    "            \"values\": [0, 0.05, 0.1, 0.2]\n",
    "        },\n",
    "        \"drop_out\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        # define model\n",
    "        model = MLPNet_3_layer(\n",
    "            fc_hidden_1=config[\"fc_hidden_1\"],\n",
    "            fc_hidden_2=config[\"fc_hidden_2\"],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"],\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "       \n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(mlp_sweep_configuration, project=\"del_mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train, count=num_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_runs = 5\n",
    "min_acc = 0.7\n",
    "\n",
    "create_sweep = False #create new sweeps\n",
    "\n",
    "cnn_sweep_configuration = {\n",
    "    \"name\": \"CNN_sweep\",\n",
    "    \"metric\": {\"name\": \"test accuracy\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"name\": {\n",
    "            \"value\": \"CNN_SGD_L2\"\n",
    "        },\n",
    "        \"conv1_channels\": {\n",
    "            \"values\":[4, 8, 16, 32]\n",
    "        },\n",
    "        \"conv2_channels\": {\n",
    "            \"values\":[4, 8, 16, 32]\n",
    "        },\n",
    "        \"fc_hidden_1\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":190,\n",
    "            \"min\":150\n",
    "        },\n",
    "        \"fc_hidden_2\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":190,\n",
    "            \"min\":50\n",
    "        },\n",
    "        \"num_epochs\": {\n",
    "            \"value\": num_epochs\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.1, 0.05]\n",
    "        },\n",
    "        \"l2_weight_decay\": {\n",
    "            \"values\": [0, 0.05, 0.1, 0.2]\n",
    "        },\n",
    "        \"drop_out\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "        \"kernel_size\": {\n",
    "            \"value\": 5\n",
    "        },\n",
    "        \"pool_size\": {\n",
    "            \"value\": 2\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        # define model\n",
    "        model = ConvNet(\n",
    "            kernel_size=config[\"kernel_size\"],\n",
    "            pool_size=config[\"pool_size\"],\n",
    "            conv1_channels=config[\"conv1_channels\"],\n",
    "            conv2_channels=config[\"conv2_channels\"],\n",
    "            fc_hidden_1=config[\"fc_hidden_1\"],\n",
    "            fc_hidden_2=config[\"fc_hidden_2\"],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"],\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "       \n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(cnn_sweep_configuration, project=\"del_mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train, count=num_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** The effect of L2 regularisation is clearly visible in the training loss. Using the L2 regularisation parameter prevents the training loss from completely converging to 0. The higher the value of the regularisation parameter, the earlier the conversion stops. However, L2 regularisation did not improve the test loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop out Regulatization\n",
    "\n",
    "With drop out regularization, a specified percentage of the output values of the nodes within a layer are swept out and their value is set to zero. The selection of the nodes occurs randomly and is repeated for each batch. By ignoring individual node outputs in training, it ensures that the model cannot solely rely on individual nodes for making a classification. This causes the individual edge weights to be lowered overall, which should produce an effect similar to that of L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_runs = 5\n",
    "min_acc = 0.7\n",
    "\n",
    "create_sweep = False #create new sweeps\n",
    "\n",
    "\n",
    "mlp_sweep_configuration = {\n",
    "    \"name\": \"MLP_sweep\",\n",
    "    \"metric\": {\"name\": \"test accuracy\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"name\": {\n",
    "            \"value\": \"MLP_SGD_DP\"\n",
    "        },\n",
    "        \"fc_hidden_1\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":200,\n",
    "            \"min\":100\n",
    "        },\n",
    "        \"fc_hidden_2\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":180,\n",
    "            \"min\":70\n",
    "        },\n",
    "        \"num_epochs\": {\n",
    "            \"value\": num_epochs\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.1, 0.05]\n",
    "        },\n",
    "        \"l2_weight_decay\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "        \"drop_out\": {\n",
    "            \"values\": [0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        # define model\n",
    "        model = MLPNet_3_layer(\n",
    "            fc_hidden_1=config[\"fc_hidden_1\"],\n",
    "            fc_hidden_2=config[\"fc_hidden_2\"],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"],\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "       \n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(mlp_sweep_configuration, project=\"del_mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train, count=num_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_runs = 5\n",
    "min_acc = 0.7\n",
    "\n",
    "create_sweep = False #create new sweeps\n",
    "\n",
    "cnn_sweep_configuration = {\n",
    "    \"name\": \"CNN_sweep\",\n",
    "    \"metric\": {\"name\": \"test accuracy\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"name\": {\n",
    "            \"value\": \"CNN_SGD_DP\"\n",
    "        },\n",
    "        \"conv1_channels\": {\n",
    "            \"values\":[4, 8, 16, 32]\n",
    "        },\n",
    "        \"conv2_channels\": {\n",
    "            \"values\":[4, 8, 16, 32]\n",
    "        },\n",
    "        \"fc_hidden_1\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":190,\n",
    "            \"min\":150\n",
    "        },\n",
    "        \"fc_hidden_2\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":190,\n",
    "            \"min\":50\n",
    "        },\n",
    "        \"num_epochs\": {\n",
    "            \"value\": num_epochs\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.1, 0.05]\n",
    "        },\n",
    "        \"l2_weight_decay\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "        \"drop_out\": {\n",
    "            \"values\": [0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "        },\n",
    "        \"kernel_size\": {\n",
    "            \"value\": 5\n",
    "        },\n",
    "        \"pool_size\": {\n",
    "            \"value\": 2\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        # define model\n",
    "        model = ConvNet(\n",
    "            kernel_size=config[\"kernel_size\"],\n",
    "            pool_size=config[\"pool_size\"],\n",
    "            conv1_channels=config[\"conv1_channels\"],\n",
    "            conv2_channels=config[\"conv2_channels\"],\n",
    "            fc_hidden_1=config[\"fc_hidden_1\"],\n",
    "            fc_hidden_2=config[\"fc_hidden_2\"],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"],\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "       \n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(cnn_sweep_configuration, project=\"del_mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train, count=num_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including Batchnorm BN (without REG)\n",
    "\n",
    "Batchnormalization is a popular method to improve the training of deep neural networks, where there is an effect that can occur with an extended number of layers: the so-called *internal covariance shift*. This describes the phenomenon when the distribution of the activation values of a layer is changed by adjusting the weights of the prior layer. The adjustments of all preceding layers thus have an influence on the distributions of all activations of subsequent layers. The subsequent layers therefore must adapt to these changes. As a result, the training is significantly slowed down.\n",
    "\n",
    "Batch normalisation aims, as the name suggests, to normalise the distribution of activation values per batch for each layer, which reduces the variation of activation values per layer to a more defined range. Batch normalisation can thus also be seen as a kind of smoothing of the cost function in each layer, which in turn reduces the variation of the gradient lengths and thus leads to a more stable learning behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet_3_layer_bn(nn.Module):\n",
    "    '''Multi Layer Perceptron Net class with 3 layers'''\n",
    "    def __init__(self, fc_hidden_1, fc_hidden_2, fc_hidden_3=None, activation=F.relu, drop_out=0):\n",
    "        super(MLPNet_3_layer_bn, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=32*32*3, out_features=fc_hidden_1)\n",
    "        self.fc1_bn = nn.BatchNorm1d(fc_hidden_1)\n",
    "        self.fc2 = nn.Linear(in_features=fc_hidden_1, out_features=fc_hidden_2)\n",
    "        self.fc2_bn = nn.BatchNorm1d(fc_hidden_2)\n",
    "        self.fc3 = nn.Linear(in_features=fc_hidden_2, out_features=10)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.activation = activation # activation function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size  = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.activation(self.fc1_bn(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2_bn(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet_bn(nn.Module):\n",
    "    def __init__(self, activation=F.relu, kernel_size=5, pool_size=2, conv1_channels = 10, conv2_channels=8, fc_hidden_1=120, fc_hidden_2=84, drop_out=0):\n",
    "        '''Convolutional Net class'''\n",
    "        super(ConvNet_bn, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=conv1_channels, kernel_size=kernel_size) \n",
    "        self.conv1_bn=nn.BatchNorm2d(conv1_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "        self.conv2 = nn.Conv2d(in_channels=conv1_channels, out_channels=conv2_channels, kernel_size=kernel_size) \n",
    "        self.conv2_bn=nn.BatchNorm2d(conv2_channels)\n",
    "        self.fc1 = nn.Linear(in_features=conv2_channels*5*5, out_features=fc_hidden_1)\n",
    "        self.fc1_bn = nn.BatchNorm1d(fc_hidden_1)\n",
    "        self.fc2 = nn.Linear(in_features=fc_hidden_1, out_features=fc_hidden_2)\n",
    "        self.fc2_bn = nn.BatchNorm1d(fc_hidden_2)\n",
    "        self.fc3 = nn.Linear(in_features=fc_hidden_2, out_features=10)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.activation = activation\n",
    "        self.conv2_channels = conv2_channels\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1_bn(self.conv1(x))))\n",
    "        x = self.pool(self.activation(self.conv2_bn(self.conv2(x))))\n",
    "        x = x.view(-1, self.conv2_channels*5*5) # flatten tensor\n",
    "        x = self.activation(self.fc1_bn(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2_bn(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvNet_3_bn(nn.Module):\n",
    "    def __init__(self, activation=F.relu, kernel_size=5, pool_size=2, padding=3, conv1_channels = 10, conv2_channels=8, conv3_channels=8, fc_hidden_1=120, fc_hidden_2=84, drop_out=0):\n",
    "        '''A convolutional neural network with 3 convolutional layers'''\n",
    "        super(ConvNet_3_bn, self).__init__()   \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=conv1_channels, kernel_size=kernel_size, padding=padding) \n",
    "        self.conv1_bn=nn.BatchNorm2d(conv1_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "        self.conv2 = nn.Conv2d(in_channels=conv1_channels, out_channels=conv2_channels, kernel_size=kernel_size, padding=padding) \n",
    "        self.conv2_bn=nn.BatchNorm2d(conv2_channels)\n",
    "        self.conv3 = nn.Conv2d(in_channels=conv2_channels, out_channels=conv3_channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv3_bn=nn.BatchNorm2d(conv3_channels)\n",
    "        self.fc1 = nn.Linear(in_features=conv3_channels*5*5, out_features=fc_hidden_1)   \n",
    "        self.fc1_bn = nn.BatchNorm1d(fc_hidden_1)\n",
    "        self.fc2 = nn.Linear(in_features=fc_hidden_1, out_features=fc_hidden_2)\n",
    "        self.fc2_bn = nn.BatchNorm1d(fc_hidden_2)\n",
    "        self.fc3 = nn.Linear(in_features=fc_hidden_2, out_features=10)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.activation = activation\n",
    "        self.conv3_channels = conv3_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1_bn(self.conv1(x))))\n",
    "        x = self.pool(self.activation(self.conv2_bn(self.conv2(x))))\n",
    "        x = self.pool(self.activation(self.conv3_bn(self.conv3(x))))\n",
    "        x = x.view(-1, self.conv3_channels*5*5) # flatten tensor\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_runs = 5\n",
    "min_acc = 0.7\n",
    "\n",
    "create_sweep = False #create new sweeps\n",
    "\n",
    "\n",
    "mlp_sweep_configuration = {\n",
    "    \"name\": \"MLP_sweep\",\n",
    "    \"metric\": {\"name\": \"test accuracy\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"name\": {\n",
    "            \"value\": \"MLP_SGD_BN\"\n",
    "        },\n",
    "        \"fc_hidden_1\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":200,\n",
    "            \"min\":100\n",
    "        },\n",
    "        \"fc_hidden_2\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":180,\n",
    "            \"min\":70\n",
    "        },\n",
    "        \"num_epochs\": {\n",
    "            \"value\": num_epochs\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.1, 0.05]\n",
    "        },\n",
    "        \"l2_weight_decay\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "        \"drop_out\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        # define model\n",
    "        model = MLPNet_3_layer_bn(\n",
    "            fc_hidden_1=config[\"fc_hidden_1\"],\n",
    "            fc_hidden_2=config[\"fc_hidden_2\"],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"],\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "       \n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(mlp_sweep_configuration, project=\"del_mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train, count=num_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_runs = 5\n",
    "min_acc = 0.7\n",
    "\n",
    "create_sweep = False #create new sweeps\n",
    "\n",
    "cnn_sweep_configuration = {\n",
    "    \"name\": \"CNN_sweep\",\n",
    "    \"metric\": {\"name\": \"test accuracy\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"name\": {\n",
    "            \"value\": \"CNN_SGD_BN\"\n",
    "        },\n",
    "        \"conv1_channels\": {\n",
    "            \"values\":[4, 8, 16, 32]\n",
    "        },\n",
    "        \"conv2_channels\": {\n",
    "            \"values\":[4, 8, 16, 32]\n",
    "        },\n",
    "        \"fc_hidden_1\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":190,\n",
    "            \"min\":150\n",
    "        },\n",
    "        \"fc_hidden_2\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":190,\n",
    "            \"min\":50\n",
    "        },\n",
    "        \"num_epochs\": {\n",
    "            \"value\": num_epochs\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.1, 0.05]\n",
    "        },\n",
    "        \"l2_weight_decay\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "        \"drop_out\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "        \"kernel_size\": {\n",
    "            \"value\": 5\n",
    "        },\n",
    "        \"pool_size\": {\n",
    "            \"value\": 2\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        # define model\n",
    "        model = ConvNet_bn(\n",
    "            kernel_size=config[\"kernel_size\"],\n",
    "            pool_size=config[\"pool_size\"],\n",
    "            conv1_channels=config[\"conv1_channels\"],\n",
    "            conv2_channels=config[\"conv2_channels\"],\n",
    "            fc_hidden_1=config[\"fc_hidden_1\"],\n",
    "            fc_hidden_2=config[\"fc_hidden_2\"],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"],\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "       \n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(cnn_sweep_configuration, project=\"del_mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train, count=num_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Adam (without BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_runs = 5\n",
    "min_acc = 0.7\n",
    "create_sweep = False #create new sweeps\n",
    "\n",
    "\n",
    "mlp_sweep_configuration = {\n",
    "    \"name\": \"MLP_sweep\",\n",
    "    \"metric\": {\"name\": \"test accuracy\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"name\": {\n",
    "            \"value\": \"MLP_SGD_ADAM\"\n",
    "        },\n",
    "        \"fc_hidden_1\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":200,\n",
    "            \"min\":80\n",
    "        },\n",
    "        \"fc_hidden_2\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":180,\n",
    "            \"min\":50\n",
    "        },\n",
    "        \"fc_hidden_3\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":180,\n",
    "            \"min\":50\n",
    "        },\n",
    "        \"num_epochs\": {\n",
    "            \"value\": num_epochs\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.1, 0.05]\n",
    "        },\n",
    "        \"l2_weight_decay\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "        \"drop_out\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        # define model\n",
    "        model = MLPNet_4_layer(\n",
    "            fc_hidden_1=config[\"fc_hidden_1\"],\n",
    "            fc_hidden_2=config[\"fc_hidden_2\"],\n",
    "            fc_hidden_3=config[\"fc_hidden_3\"],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.Adam(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"],\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "       \n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(mlp_sweep_configuration, project=\"del_mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train, count=num_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_runs = 5\n",
    "min_acc = 0.7\n",
    "create_sweep = False #create new sweeps\n",
    "\n",
    "cnn_sweep_configuration = {\n",
    "    \"name\": \"CNN_sweep\",\n",
    "    \"metric\": {\"name\": \"test accuracy\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"name\": {\n",
    "            \"value\": \"CNN_SGD_ADAM\"\n",
    "        },\n",
    "        \"conv1_channels\": {\n",
    "            \"values\":[32, 64, 128]\n",
    "        },\n",
    "        \"conv2_channels\": {\n",
    "            \"values\":[32, 64, 128]\n",
    "        },\n",
    "        \"conv3_channels\": {\n",
    "            \"values\":[32, 64, 128]\n",
    "        },\n",
    "        \"fc_hidden_1\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":190,\n",
    "            \"min\":150\n",
    "        },\n",
    "        \"fc_hidden_2\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"max\":190,\n",
    "            \"min\":50\n",
    "        },\n",
    "        \"num_epochs\": {\n",
    "            \"value\": num_epochs\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.1, 0.05]\n",
    "        },\n",
    "        \"l2_weight_decay\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "        \"drop_out\": {\n",
    "            \"value\": 0\n",
    "        },\n",
    "        \"kernel_size\": {\n",
    "            \"value\": 5\n",
    "        },\n",
    "        \"pool_size\": {\n",
    "            \"value\": 2\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(entity=\"simonluder\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        wandb.run.name = \"{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        # define model\n",
    "        model = ConvNet_3(\n",
    "            kernel_size=config[\"kernel_size\"],\n",
    "            pool_size=config[\"pool_size\"],\n",
    "            conv1_channels=config[\"conv1_channels\"],\n",
    "            conv2_channels=config[\"conv2_channels\"],\n",
    "            conv3_channels=config[\"conv3_channels\"],\n",
    "            fc_hidden_1=config[\"fc_hidden_1\"],\n",
    "            fc_hidden_2=config[\"fc_hidden_2\"],\n",
    "            drop_out=config[\"drop_out\"],\n",
    "            ).to(device) \n",
    "        \n",
    "        # define trainer\n",
    "        trainer = ModelTrainer(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            test_loader=test_loader, \n",
    "            num_epochs=config[\"num_epochs\"],\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            optimizer=torch.optim.Adam(\n",
    "                model.parameters(), \n",
    "                lr=config[\"learning_rate\"],\n",
    "                weight_decay = config[\"l2_weight_decay\"],\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        #train model\n",
    "        trainer.train(verbose=False, include_test_scores=True)\n",
    "\n",
    "        # save model if minimal test accuracy-score is reached\n",
    "        if trainer.best_log[\"test accuracy\"] >= min_acc:\n",
    "            save_model(path=\"top_model\", model=trainer.best_model, name=wandb.run.name)\n",
    "       \n",
    "if create_sweep:\n",
    "    sweep_id = wandb.sweep(cnn_sweep_configuration, project=\"del_mc1\", entity=\"simonluder\")\n",
    "    wandb.agent(sweep_id, function=train, count=num_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first noticeable change is the change in the optimal learning rate. While our previous variants had an optimal learning rate between 0.1 and 0.05, with the Adam optimizer this rate is clearly too high and leads to an unstable learning behaviour. It seems that for our models in the CIFAR10 data set, a 10 times smaller learning rate is necessary to achieve a stable training behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53a60aad9525bfb42eeb15304a4347f0069f2e83056fd26ff65070a66e6c6e0c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
